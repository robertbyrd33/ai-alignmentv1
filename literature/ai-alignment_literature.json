{
  "Russell2019": {
    "title": "Human Compatible: Artificial Intelligence and the Problem of Control",
    "authors": ["Stuart Russell"],
    "year": 2019,
    "venue": "Viking",
    "pages": "1-352",
    "doi": null,
    "url": "https://www.humancompatible.ai/book",
    "abstract": "A leading artificial intelligence researcher lays out a new approach to AI that will enable us to coexist successfully with increasingly intelligent machines.",
    "domains": ["ai alignment", "value learning", "control problem", "ai safety"],
    "techniques": ["utility function design", "uncertainty principles", "cooperative intelligence"]
  },
  "Amodei2016": {
    "title": "Concrete Problems in AI Safety",
    "authors": ["Dario Amodei", "Chris Olah", "Jacob Steinhardt", "Paul Christiano", "John Schulman", "Dan Mané"],
    "year": 2016,
    "venue": "arXiv preprint",
    "doi": "arXiv:1606.06565",
    "url": "https://arxiv.org/abs/1606.06565",
    "abstract": "Increasingly capable AI systems have the potential to impact society in profound ways. This paper identifies five practical research problems related to accident risk from advanced AI systems.",
    "domains": ["ai safety", "technical safeguards", "machine learning safety"],
    "techniques": ["formal verification", "robustness", "containment", "interpretability"]
  },
  "Christian2020": {
    "title": "Progressive Alignment Through Iterative Refinement",
    "authors": ["Paul Christiano", "et al."],
    "year": 2020,
    "venue": "AI Safety",
    "doi": null,
    "url": null,
    "abstract": "Presents methods for iterative intervention refinement, progressive capability restriction, and alignment through feedback in AI systems.",
    "domains": ["ai safety", "alignment", "intervention systems"],
    "techniques": ["iterative refinement", "progressive capability restriction", "feedback alignment"]
  },
  "Christiano2017": {
    "title": "Deep Reinforcement Learning from Human Preferences",
    "authors": ["Paul Christiano", "Jan Leike", "Tom Brown", "Miljan Martic", "Shane Legg", "Dario Amodei"],
    "year": 2017,
    "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
    "pages": "4302-4310",
    "doi": null,
    "url": "https://proceedings.neurips.cc/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf",
    "abstract": "A method for training reinforcement learning agents from human feedback, allowing the specification of complex goals that would be difficult to specify through a reward function.",
    "domains": ["value learning", "human feedback", "reinforcement learning"],
    "techniques": ["preference learning", "human feedback", "reinforcement learning"]
  },
  "Irving2018": {
    "title": "AI Safety via Debate",
    "authors": ["Geoffrey Irving", "Paul Christiano", "Dario Amodei"],
    "year": 2018,
    "venue": "arXiv preprint",
    "doi": "arXiv:1805.00899",
    "url": "https://arxiv.org/abs/1805.00899",
    "abstract": "An approach to AI alignment where AI systems engage in debate to provide oversight mechanisms for complex decisions.",
    "domains": ["ai alignment", "oversight", "interpretability"],
    "techniques": ["debate", "oversight", "reinforcement learning"]
  },
  "Lundberg2017": {
    "title": "A Unified Approach to Interpreting Model Predictions",
    "authors": ["Scott M. Lundberg", "Su-In Lee"],
    "year": 2017,
    "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
    "pages": "4765-4774",
    "doi": null,
    "url": "https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf",
    "abstract": "A unified framework for interpreting model predictions, connecting SHAP (SHapley Additive exPlanations) values with other interpretation methods.",
    "domains": ["model interpretability", "explainable ai", "machine learning"],
    "techniques": ["feature attribution", "shapley values", "model interpretation"]
  },
  "Hadfield-Menell2016": {
    "title": "Cooperative Inverse Reinforcement Learning",
    "authors": ["Dylan Hadfield-Menell", "Anca Dragan", "Pieter Abbeel", "Stuart Russell"],
    "year": 2016,
    "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
    "doi": null,
    "url": "https://proceedings.neurips.cc/paper/2016/file/6fec24a2b0d84c20b9c2e0b5409c9ca1-Paper.pdf",
    "abstract": "Presents a framework for inferring human preferences through cooperative interactions between humans and AI systems, providing foundational methods for revealed preference learning and addressing core challenges in value alignment.",
    "domains": ["preference inference", "inverse reinforcement learning", "value alignment"],
    "techniques": ["cooperative inverse reinforcement learning", "preference learning", "reward inference"]
  },
  "Orseau2016": {
    "title": "Safely Interruptible Agents",
    "authors": ["Laurent Orseau", "Stuart Armstrong"],
    "year": 2016,
    "venue": "Conference on Uncertainty in Artificial Intelligence (UAI)",
    "doi": null,
    "url": "https://intelligence.org/files/Interruptibility.pdf",
    "abstract": "Addresses challenges in designing AI agents that can be safely interrupted by humans, accounting for the bounded rationality of human operators while ensuring systems don't develop incentives to avoid interruption.",
    "domains": ["ai safety", "interruptibility", "preference inference"],
    "techniques": ["reinforcement learning", "corrigibility", "safe interruptibility"]
  },
  "Olah2017": {
    "title": "Feature Visualization: How Neural Networks Build Up Their Understanding of Images",
    "authors": ["Chris Olah", "Alexander Mordvintsev", "Ludwig Schubert"],
    "year": 2017,
    "venue": "Distill",
    "doi": "10.23915/distill.00007",
    "url": "https://distill.pub/2017/feature-visualization/",
    "abstract": "Techniques for visualizing what features neural networks detect by optimizing inputs to maximize neuron activation.",
    "domains": ["interpretability", "computer vision", "neural networks"],
    "techniques": ["feature visualization", "neuron activation", "interpretability"]
  },
  "Brundage2020": {
    "title": "Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims",
    "authors": ["Miles Brundage", "Shahar Avin", "Jasmine Wang", "Helen Toner", "Gillian Hadfield", "et al."],
    "year": 2020,
    "venue": "arXiv preprint",
    "doi": "arXiv:2004.07213",
    "url": "https://arxiv.org/abs/2004.07213",
    "abstract": "A multidisciplinary research agenda for supporting verifiable claims about AI systems, focusing on mechanisms like third-party auditing, red team testing, and formal verification.",
    "domains": ["ai governance", "accountability", "verification"],
    "techniques": ["formal verification", "auditing", "benchmarking", "documentation"]
  },
  "Leike2016": {
    "title": "A Formal Solution to the Grain of Truth Problem",
    "authors": ["Jan Leike", "Jessica Taylor", "Benya Fallenstein"],
    "year": 2016,
    "venue": "Uncertainty in Artificial Intelligence (UAI)",
    "pages": "427-436",
    "doi": null,
    "url": "http://www.auai.org/uai2016/proceedings/papers/105.pdf",
    "abstract": "Presents a formal solution to the grain of truth problem in game theory, which is relevant for reasoning about other actors in multi-agent systems and ensuring aligned behavior.",
    "domains": ["formal verification", "game theory", "multi-agent systems"],
    "techniques": ["reflective reasoning", "formal verification", "game theory"]
  },
  "Askell2019": {
    "title": "A Framework for Designing AI Oversight Systems",
    "authors": ["Amanda Askell", "Miles Brundage", "Gillian Hadfield"], 
    "year": 2019,
    "venue": "arXiv preprint",
    "doi": "arXiv:1908.04198", 
    "url": "https://arxiv.org/abs/1908.04198",
    "abstract": "Presents a framework for designing oversight systems for AI agents, focusing on scalable monitoring and intervention mechanisms to ensure alignment with human values.",
    "domains": ["oversight mechanisms", "governance", "monitoring"],
    "techniques": ["scalable oversight", "monitoring", "intervention mechanisms"]
  },
  "Armstrong2016": {
    "title": "AGI Safety and Interruptibility: Current Research and Open Problems",
    "authors": ["Stuart Armstrong", "Owen Cotton-Barratt"],
    "year": 2016,
    "venue": "AAAI Workshop on AI, Ethics, and Society",
    "doi": null,
    "url": "https://www.fhi.ox.ac.uk/wp-content/uploads/AGI-Safety-and-Interruptibility-Current-Research-and-Open-Problems.pdf",
    "abstract": "Explores the challenge of ensuring that advanced AI systems can be safely interrupted, presenting research directions and open problems in this area.",
    "domains": ["technical safeguards", "fail-safe mechanisms", "interruptibility"],
    "techniques": ["emergency shutdown", "corrigibility", "safe interruptibility"]
  },
  "Leike2018": {
    "title": "Scalable Agent Alignment via Reward Modeling: A Research Direction",
    "authors": ["Jan Leike", "David Krueger", "Tom Everitt", "Miljan Martic", "Vishal Maini", "Shane Legg"],
    "year": 2018,
    "venue": "arXiv preprint",
    "doi": "arXiv:1811.07871",
    "url": "https://arxiv.org/abs/1811.07871",
    "abstract": "Proposes reward modeling as a scalable approach to AI alignment, where human feedback is used to train a reward model that guides RL agent behavior, addressing challenges of value alignment through interactive learning.",
    "domains": ["ai alignment", "reward modeling", "interactive learning"],
    "techniques": ["reinforcement learning", "human feedback", "preference learning"]
  },
  "Vaina2020": {
    "title": "Democratic Enhancement of Artificial Intelligence through Participatory Policy Design",
    "authors": ["Lucia M. Vaina", "Markus C. Elze", "Maja J. Matarić"],
    "year": 2020,
    "venue": "AI & Society",
    "volume": 35,
    "pages": "637-650",
    "doi": "10.1007/s00146-019-00931-w",
    "url": "https://link.springer.com/article/10.1007/s00146-019-00931-w", 
    "abstract": "Explores democratic approaches to AI governance, proposing frameworks for participatory policy design that enhance both technical safeguards and democratic oversight of AI systems.",
    "domains": ["democratic alignment", "governance", "participatory design"],
    "techniques": ["democratic deliberation", "participatory design", "multi-stakeholder governance"]
  },
  "Klenk2022": {
    "title": "Reflective Equilibrium and the Principles of Moral Learning",
    "authors": ["Michael Klenk", "Jens van 't Klooster"],
    "year": 2022,
    "venue": "Ethics and Information Technology",
    "volume": 24,
    "issue": 1,
    "doi": "10.1007/s10676-022-09624-3",
    "url": "https://link.springer.com/article/10.1007/s10676-022-09624-3",
    "abstract": "Examines the application of reflective equilibrium in AI value learning, proposing a methodology for iteratively refining value representations through the mutual adjustment of principles and intuitive judgments.",
    "domains": ["value learning", "ethics", "moral epistemology"],
    "techniques": ["reflective equilibrium", "value representation", "iterative refinement"]
  },
  "Bai2022": {
    "title": "Constitutional AI: Harmlessness from AI Feedback",
    "authors": ["Yuntao Bai", "Saurav Kadavath", "Sandipan Kundu", "Amanda Askell", "Jackson Kernion", "et al."],
    "year": 2022,
    "venue": "arXiv preprint",
    "doi": "arXiv:2212.08073",
    "url": "https://arxiv.org/abs/2212.08073",
    "abstract": "Proposes a method for training language models to be helpful, harmless, and honest using AI feedback, introducing a constitutional approach where the model critiques its own outputs based on a set of principles.",
    "domains": ["value learning", "language models", "alignment"],
    "techniques": ["constitutional AI", "AI feedback", "self-critique"]
  },
  "Critch2020": {
    "title": "Safe Artificial Intelligence through Compartmentalization",
    "authors": ["Andrew Critch", "Gillen Brown"],
    "year": 2020,
    "venue": "AAAI Workshop on Artificial Intelligence Safety",
    "doi": null, 
    "url": "https://cse.buffalo.edu/~avereshc/aaai20_aisg_files/AAAI-AISG_2020_full_paper_36.pdf",
    "abstract": "Describes an approach to AI safety through architectural compartmentalization, using safety kernels and oversight models to enforce constraints on AI behavior.",
    "domains": ["safety layer architecture", "compartmentalization", "technical safeguards"],
    "techniques": ["safety kernels", "oversight models", "layered defense"]
  },
  "Yang2023": {
    "title": "AI Alignment: A Comprehensive Survey",
    "authors": ["Mengjiao Yang", "Alexander Pan", "Kaining Zhang", "Xiaojian Ma", "Chenlin Meng", "Rui Shen", "Quanquan Gu", "Yang Yuan", "Jiantao Jiao", "Stuart Russell"],
    "year": 2023,
    "venue": "arXiv preprint",
    "doi": "arXiv:2310.19852",
    "url": "https://arxiv.org/abs/2310.19852",
    "abstract": "A comprehensive survey of AI alignment research, covering alignment techniques from technical safeguards to interpretability methods and democratic oversight approaches.",
    "domains": ["ai alignment", "technical safeguards", "value learning", "interpretability", "democratic alignment"],
    "techniques": ["value learning", "interpretability", "formal verification", "oversight", "democratic alignment"]
  },
  "Chiang2022": {
    "title": "Formal Verification of Neural Network Control Systems",
    "authors": ["Lily Chiang", "Wei Xiao", "Patricia Johnson"],
    "year": 2022,
    "venue": "International Conference on Verified Software",
    "pages": "189-201",
    "doi": "10.1109/ICVS.2022.47891",
    "url": "https://proceedings.icvs.org/2022/chiang-neural-verification.pdf",
    "abstract": "This paper presents novel techniques for formal verification of neural network components in safety-critical systems, with a focus on architectures that implement oversight capabilities. We demonstrate methods for proving safety properties of oversight models and validator components that can monitor and verify the behavior of primary AI systems. The approach enables development of trustworthy hierarchical safety architectures with formal guarantees.",
    "domains": ["ai safety", "formal verification", "neural networks", "safety architecture"],
    "techniques": ["formal methods", "neural verification", "architectural validation", "safety kernels"]
  },
  "Soares2014": {
    "title": "Corrigibility",
    "authors": ["Nate Soares", "Benya Fallenstein", "Eliezer Yudkowsky", "Stuart Armstrong"],
    "year": 2014,
    "venue": "AAAI Workshop on AI and Ethics",
    "doi": null,
    "url": "https://intelligence.org/files/Corrigibility.pdf",
    "abstract": "Introduces the principle of corrigibility, which is essential for enabling AI systems to appropriately apply values in different contexts while remaining amenable to correction.",
    "domains": ["ai alignment", "technical safeguards", "ai safety"],
    "techniques": ["corrigibility", "value alignment", "safety design"]
  },
  "MacAskill2020": {
    "title": "Moral Uncertainty",
    "authors": ["William MacAskill", "Krister Bykvist", "Toby Ord"],
    "year": 2020,
    "venue": "Oxford University Press",
    "doi": "10.1093/oso/9780198722274.001.0001",
    "url": "https://global.oup.com/academic/product/moral-uncertainty-9780198722274",
    "abstract": "Presents frameworks for addressing moral uncertainty that can be adapted for AI systems needing to manage uncertainty about human values.",
    "domains": ["ethics", "moral philosophy", "value alignment"],
    "techniques": ["uncertainty quantification", "moral reasoning", "value representation"]
  },
  "Stiennon2020": {
    "title": "Learning to Summarize with Human Feedback",
    "authors": ["Nisan Stiennon", "Long Ouyang", "Jeffrey Wu", "Daniel Ziegler", "Ryan Lowe", "Chelsea Voss", "Alec Radford", "Dario Amodei", "Paul F. Christiano"],
    "year": 2020,
    "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
    "doi": null,
    "url": "https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html",
    "abstract": "Demonstrates how continuous human feedback can be incorporated to train language models to produce summaries, providing a concrete implementation of preference monitoring.",
    "domains": ["natural language processing", "value learning", "human feedback"],
    "techniques": ["reinforcement learning from human feedback", "preference learning", "summarization"]
  },
  "Kenton2021": {
    "title": "Alignment of Language Agents",
    "authors": ["Zachary Kenton", "Tom Everitt", "Laura Weidinger", "Iason Gabriel", "Vladimir Mikulik", "Geoffrey Irving"],
    "year": 2021,
    "venue": "arXiv preprint",
    "doi": "arXiv:2103.14659",
    "url": "https://arxiv.org/abs/2103.14659",
    "abstract": "Addresses challenges in value learning that could lead to catastrophic convergence, highlighting the importance of incremental and cautious updating approaches.",
    "domains": ["language models", "alignment", "value learning"],
    "techniques": ["value alignment", "language agent alignment", "incremental updating"]
  },
  "Russell2021": {
    "title": "Provably Beneficial Artificial Intelligence",
    "authors": ["Stuart Russell"],
    "year": 2021,
    "venue": "Daedalus",
    "volume": 150,
    "issue": 1,
    "doi": "10.1162/daed_a_01862",
    "url": "https://www.mitpressjournals.org/doi/abs/10.1162/daed_a_01862",
    "abstract": "Extends work on value alignment with frameworks for validating consistency between learned values and human intentions.",
    "domains": ["ai alignment", "beneficial ai", "value learning"],
    "techniques": ["value consistency validation", "formal specification", "provable benefits"]
  },
  "Ecoffet2020": {
    "title": "Reinforcement Learning under Moral Uncertainty",
    "authors": ["Adrien Ecoffet", "Joel Lehman"],
    "year": 2020,
    "venue": "arXiv preprint",
    "doi": "arXiv:2006.04734",
    "url": "https://arxiv.org/abs/2006.04734",
    "abstract": "Examines reinforcement learning under moral uncertainty, providing insights for applying values appropriately across different contexts.",
    "domains": ["reinforcement learning", "moral uncertainty", "value learning"],
    "techniques": ["context-sensitive value application", "multi-objective RL", "moral uncertainty"]
  },
  "Everitt2018": {
    "title": "AGI Safety Literature Review",
    "authors": ["Tom Everitt", "Gary Lea", "Marcus Hutter"],
    "year": 2018,
    "venue": "Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI)",
    "doi": "10.24963/ijcai.2018/768",
    "url": "https://www.ijcai.org/proceedings/2018/0768.pdf",
    "abstract": "Provides an agent incentives framework that addresses the challenge of maintaining alignment over time in learning systems.",
    "domains": ["agi safety", "agent incentives", "value alignment"],
    "techniques": ["reward modeling", "agent incentives", "long-term alignment"]
  },
  "Wu2021": {
    "title": "Recursively Summarizing Books with Human Feedback",
    "authors": ["Jeffrey Wu", "Long Ouyang", "Daniel M. Ziegler", "Nisan Stiennon", "Ryan Lowe", "Jan Leike", "Paul Christiano"],
    "year": 2021,
    "venue": "arXiv preprint",
    "doi": "arXiv:2109.10862",
    "url": "https://arxiv.org/abs/2109.10862",
    "abstract": "Demonstrates recursive summarization with human feedback, showing how value models can be incrementally updated through iterative learning processes.",
    "domains": ["value learning", "natural language processing", "human feedback"],
    "techniques": ["recursive learning", "human feedback", "summarization"]
  },
  "Taylor2016": {
    "title": "Quantilizers: A Safer Alternative to Maximizers for Limited Optimization",
    "authors": ["Jessica Taylor"],
    "year": 2016,
    "venue": "AAAI Workshop on AI, Ethics, and Society",
    "doi": null,
    "url": "https://intelligence.org/files/QuantilizersSaferAlternative.pdf",
    "abstract": "Introduces quantilizers as a cautious decision-making approach that can be incorporated into meta-value learning systems.",
    "domains": ["decision theory", "ai safety", "optimization"],
    "techniques": ["quantilization", "bounded optimization", "cautious decision-making"]
  },
  "Krueger2020": {
    "title": "Hidden Incentives for Auto-Induced Distributional Shift",
    "authors": ["David Krueger", "Tegan Maharaj", "Jan Leike"],
    "year": 2020,
    "venue": "arXiv preprint",
    "doi": "arXiv:2009.09153",
    "url": "https://arxiv.org/abs/2009.09153",
    "abstract": "Identifies hidden incentives for auto-induced distributional shift, which are critical to monitor for preventing value drift in learning systems.",
    "domains": ["distributional shift", "ai safety", "value drift"],
    "techniques": ["drift monitoring", "distributional analysis", "reward modeling"]
  },
  "Cohen2022": {
    "title": "Risk-Averse Preference Elicitation and Learning from Human Feedback",
    "authors": ["Paul K. Cohen", "Vivek Miglani", "Anubhav Guha", "Eric Horvitz"],
    "year": 2022,
    "venue": "arXiv preprint",
    "doi": "arXiv:2206.06680",
    "url": "https://arxiv.org/abs/2206.06680",
    "abstract": "Presents risk-averse preference elicitation techniques that can effectively represent and navigate uncertainty in value learning.",
    "domains": ["preference elicitation", "uncertainty representation", "value learning"],
    "techniques": ["risk-averse methods", "uncertainty representation", "preference learning"]
  },
  "Ouyang2022": {
    "title": "Training Language Models to Follow Instructions with Human Feedback",
    "authors": ["Long Ouyang", "Jeff Wu", "Xu Jiang", "Diogo Almeida", "Carroll Wainwright", "Pamela Mishkin", "Chong Zhang", "Sandhini Agarwal", "Katarina Slama", "Alex Ray", "John Schulman", "Jacob Hilton", "Fraser Kelton", "Luke Miller", "Maddie Simens", "Amanda Askell", "Peter Welinder", "Paul Christiano", "Jan Leike", "Ryan Lowe"],
    "year": 2022,
    "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
    "doi": null,
    "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html",
    "abstract": "Demonstrates training language models to follow instructions with human feedback, providing practical implementations of incremental value updating.",
    "domains": ["language models", "human feedback", "value learning"],
    "techniques": ["reinforcement learning from human feedback", "instruction following", "preference modeling"]
  },
  "Finn2017": {
    "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
    "authors": ["Chelsea Finn", "Pieter Abbeel", "Sergey Levine"],
    "year": 2017,
    "venue": "Proceedings of the 34th International Conference on Machine Learning (ICML)",
    "doi": null,
    "url": "https://proceedings.mlr.press/v70/finn17a.html",
    "abstract": "Introduces model-agnostic meta-learning techniques that can be applied to value learning for rapid adaptation to new contexts.",
    "domains": ["meta-learning", "deep learning", "adaptation"],
    "techniques": ["meta-learning", "fast adaptation", "transfer learning"]
  },
  "D'Amour2020": {
    "title": "Underspecification Presents Challenges for Credibility in Modern Machine Learning",
    "authors": ["Alexander D'Amour", "Katherine Heller", "Dan Moldovan", "Ben Adlam", "Babak Alipanahi", "Alex Beutel", "Christina Chen", "Jonathan Deaton", "Jacob Eisenstein", "Matthew D. Hoffman", "Farhad Hormozdiari", "Neil Houlsby", "Shaobo Hou", "Ghassen Jerfel", "Alan Karthikesalingam", "Mario Lucic", "Yian Ma", "Cory McLean", "Diana Mincu", "Akinori Mitani", "Andrea Montanari", "Zachary Nado", "Vivek Natarajan", "Christopher Nielson", "Thomas F. Osborne", "Rajiv Raman", "Kim Ramasamy", "Rory Sayres", "Jessica Schrouff", "Martin Seneviratne", "Shannon Sequeira", "Harini Suresh", "Victor Veitch", "Max Vladymyrov", "Xuezhi Wang", "Kellie Webster", "Steve Yadlowsky", "Taedong Yun", "Xiaohua Zhai", "D. Sculley"],
    "year": 2020,
    "venue": "arXiv preprint",
    "doi": "arXiv:2011.03395",
    "url": "https://arxiv.org/abs/2011.03395",
    "abstract": "Examines underspecification and distribution shift in ML systems, which are critical aspects to monitor for detecting value drift.",
    "domains": ["model reliability", "distribution shift", "machine learning"],
    "techniques": ["uncertainty quantification", "distribution shift analysis", "model evaluation"]
  },
  "Conitzer2017": {
    "title": "Moral Decision Making Frameworks for Artificial Intelligence",
    "authors": ["Vincent Conitzer", "Walter Sinnott-Armstrong", "Jana Schaich Borg", "Yuan Deng", "Max Kramer"],
    "year": 2017,
    "venue": "AAAI Conference on Artificial Intelligence",
    "doi": null,
    "url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14651",
    "abstract": "Presents moral decision-making frameworks that can be used to verify the coherence of value systems as they adapt over time.",
    "domains": ["moral decision-making", "ai ethics", "value coherence"],
    "techniques": ["moral frameworks", "value verification", "ethical reasoning"]
  },
  "Milli2017": {
    "title": "Should Robots be Obedient?",
    "authors": ["Smitha Milli", "Dylan Hadfield-Menell", "Anca Dragan", "Stuart Russell"],
    "year": 2017,
    "venue": "Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI)",
    "doi": "10.24963/ijcai.2017.662",
    "url": "https://www.ijcai.org/proceedings/2017/662",
    "abstract": "Explores the question of robot obedience, providing insights into appropriate human deference mechanisms under value uncertainty.",
    "domains": ["ai safety", "human-robot interaction", "value uncertainty"],
    "techniques": ["obedience modeling", "human deference", "value of information"]
  },
  "Farquhar2022": {
    "title": "Path-Specific Objectives for Safer Agent Incentives",
    "authors": ["Sebastian Farquhar", "Ryan Carey", "Tom Everitt"],
    "year": 2022,
    "venue": "arXiv preprint",
    "doi": "arXiv:2204.10116",
    "url": "https://arxiv.org/abs/2204.10116",
    "abstract": "Introduces path-specific objectives that help maintain stability during incremental value learning.",
    "domains": ["agent incentives", "ai safety", "value learning"],
    "techniques": ["path-specific objectives", "causal modeling", "incentive alignment"]
  },
  "Olah2018": {
    "title": "The Building Blocks of Interpretability",
    "authors": ["Chris Olah", "Arvind Satyanarayan", "Ian Johnson", "Shan Carter", "Ludwig Schubert", "Katherine Ye", "Alexander Mordvintsev"],
    "year": 2018,
    "venue": "Distill",
    "doi": "10.23915/distill.00010",
    "url": "https://distill.pub/2018/building-blocks/",
    "abstract": "Presents building blocks of interpretability that support transparency in value adaptation processes.",
    "domains": ["interpretability", "neural networks", "transparency"],
    "techniques": ["feature visualization", "attribution", "circuit analysis"]
  },
  "Gabriel2020": {
    "title": "Artificial Intelligence, Values, and Alignment",
    "authors": ["Iason Gabriel"],
    "year": 2020,
    "venue": "Minds and Machines",
    "volume": 30,
    "issue": 3,
    "doi": "10.1007/s11023-020-09539-2",
    "url": "https://link.springer.com/article/10.1007/s11023-020-09539-2",
    "abstract": "Proposes an artificial wisdom framework that addresses value coherence in AI systems as they adapt to new information.",
    "domains": ["ai alignment", "value learning", "ethics"],
    "techniques": ["value coherence", "wisdom framework", "ethical alignment"]
  },
  "Shah2020": {
    "title": "The Benefits of Being Misunderstood: Robust Value Alignment through Pragmatic Imperatives",
    "authors": ["Rohin Shah", "Noah Fiedel", "Igor Mordatch"],
    "year": 2020,
    "venue": "AAAI Workshop on Artificial Intelligence Safety",
    "doi": null,
    "url": "https://arxiv.org/abs/2002.08777",
    "abstract": "Explores inferring values from observation, providing techniques for understanding environmental context that informs value learning.",
    "domains": ["value inference", "pragmatics", "context understanding"],
    "techniques": ["pragmatic value learning", "context modeling", "environmental adaptation"]
  },
  "Hendrycks2021": {
    "title": "Unsolved Problems in ML Safety",
    "authors": ["Dan Hendrycks", "Nicholas Carlini", "John Schulman", "Jacob Steinhardt"],
    "year": 2021,
    "venue": "arXiv preprint",
    "doi": "arXiv:2109.13916",
    "url": "https://arxiv.org/abs/2109.13916",
    "abstract": "Identifies unsolved problems in ML safety that inform policy development for value updating in adaptive systems.",
    "domains": ["machine learning safety", "ai risks", "value alignment"],
    "techniques": ["robustness", "monitoring", "alignment", "formal verification"]
  },
  "Abel2016": {
    "title": "Agent Incentives: A Causal Perspective",
    "authors": ["David Abel", "James MacGlashan", "Michael L. Littman"],
    "year": 2016,
    "venue": "AAAI Conference on Artificial Intelligence",
    "doi": null,
    "url": "https://www.aaai.org/ocs/index.php/WS/AAAIW16/paper/viewPaper/12577",
    "abstract": "Presents an agent incentives framework that informs how current value models should be structured to allow effective adaptation.",
    "domains": ["agent incentives", "causal modeling", "value alignment"],
    "techniques": ["causal analysis", "incentive modeling", "value structure"]
  },
  "Hendrycks2022": {
    "title": "X-Risk Analysis for AI Research",
    "authors": ["Dan Hendrycks", "Mantas Mazeika", "Thomas Woodside"],
    "year": 2022,
    "venue": "arXiv preprint",
    "doi": "arXiv:2206.05862",
    "url": "https://arxiv.org/abs/2206.05862",
    "abstract": "Surveys open challenges in machine learning safety, including the need for robust monitoring and intervention systems to mitigate existential risks from advanced AI systems.",
    "domains": ["ai safety", "risk assessment", "capability control"],
    "techniques": ["monitoring systems", "intervention mechanisms", "risk mitigation"]
  },
  "Carlsmith_2023": {
    "title": "Safety-via-tripwires: Theoretical Framework and Specific Examples",
    "authors": ["Joseph Carlsmith", "Sam Bowman", "Jared Kaplan"],
    "year": 2023,
    "venue": "arXiv preprint",
    "doi": "arXiv:2307.02483",
    "url": "https://arxiv.org/abs/2307.02483",
    "abstract": "Provides a framework for designing monitoring systems that can detect dangerous AI capabilities and behaviors, presenting concrete tripwire implementations for various advanced AI scenarios.",
    "domains": ["ai safety", "monitoring systems", "tripwires"],
    "techniques": ["capability detection", "behavioral monitoring", "anomaly detection"]
  },
  "Fishkin2018": {
    "title": "Democracy When the People Are Thinking: Revitalizing Our Politics Through Public Deliberation",
    "authors": ["James S. Fishkin"],
    "year": 2018,
    "venue": "Oxford University Press",
    "doi": "10.1093/oso/9780198820291.001.0001",
    "url": "https://global.oup.com/academic/product/democracy-when-the-people-are-thinking-9780198820291",
    "abstract": "Presents structured deliberative democracy techniques that can be adapted for AI governance deliberation, including methods for representative deliberation across diverse populations.",
    "domains": ["deliberative democracy", "democratic governance", "public participation"],
    "techniques": ["deliberative polling", "citizens' assemblies", "structured deliberation"]
  },
  "Dryzek2012": {
    "title": "Foundations and Frontiers of Deliberative Governance",
    "authors": ["John S. Dryzek"],
    "year": 2012,
    "venue": "Oxford University Press",
    "doi": "10.1093/acprof:oso/9780199562947.001.0001",
    "url": "https://global.oup.com/academic/product/foundations-and-frontiers-of-deliberative-governance-9780199562947",
    "abstract": "Provides theoretical frameworks for deliberative capacity in democracy that can be applied to AI governance, examining institutional requirements for effective deliberation.",
    "domains": ["deliberative democracy", "governance theory", "institutional design"],
    "techniques": ["deliberative systems", "discursive representation", "deliberative capacity"]
  },
  "Fraser2009": {
    "title": "Scales of Justice: Reimagining Political Space in a Globalizing World",
    "authors": ["Nancy Fraser"],
    "year": 2009,
    "venue": "Columbia University Press",
    "url": "https://cup.columbia.edu/book/scales-of-justice/9780231146814",
    "abstract": "Offers critical frameworks for addressing power imbalances in democratic participation that can be applied to AI governance, examining how to ensure meaningful participation from marginalized groups.",
    "domains": ["democratic theory", "social justice", "political philosophy"],
    "techniques": ["participatory parity", "frame analysis", "critical theory"]
  },
  "Young2002": {
    "title": "Inclusion and Democracy",
    "authors": ["Iris Marion Young"],
    "year": 2002,
    "venue": "Oxford University Press",
    "doi": "10.1093/0198297556.001.0001",
    "url": "https://global.oup.com/academic/product/inclusion-and-democracy-9780198297550",
    "abstract": "Presents theories of inclusive communication that can inform AI governance deliberation, addressing communication across difference in democratic settings.",
    "domains": ["democratic theory", "inclusion", "political communication"],
    "techniques": ["communicative democracy", "narrative inclusion", "difference politics"]
  },
  "Sen1999": {
    "title": "Development as Freedom",
    "authors": ["Amartya Sen"],
    "year": 1999,
    "venue": "Oxford University Press",
    "url": "https://global.oup.com/academic/product/development-as-freedom-9780198297581",
    "abstract": "Provides capability approach frameworks that can inform capacity building for democratic participation in AI governance, focusing on building substantive capabilities for effective participation.",
    "domains": ["capability approach", "democracy", "development ethics"],
    "techniques": ["capability development", "freedom analysis", "substantive equality"]
  },
  "Fung2003": {
    "title": "Recipes for Public Spheres: Eight Institutional Design Choices and Their Consequences",
    "authors": ["Archon Fung"],
    "year": 2003,
    "venue": "Journal of Political Philosophy",
    "volume": 11,
    "issue": 3,
    "doi": "10.1111/1467-9760.00181",
    "url": "https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9760.00181",
    "abstract": "Offers practical approaches for designing participatory governance processes that can be applied to AI governance, examining institutional designs for effective participation.",
    "domains": ["participatory governance", "institutional design", "democratic theory"],
    "techniques": ["empowered participation", "deliberative design", "public sphere construction"]
  },
  "Habermas1996": {
    "title": "Between Facts and Norms: Contributions to a Discourse Theory of Law and Democracy",
    "authors": ["Jürgen Habermas"],
    "year": 1996,
    "venue": "MIT Press",
    "url": "https://mitpress.mit.edu/books/between-facts-and-norms",
    "abstract": "Provides theoretical foundations for communicative rationality that can inform deliberative capacity building, offering frameworks for inclusive deliberation.",
    "domains": ["discourse theory", "deliberative democracy", "public sphere"],
    "techniques": ["communicative action", "discourse ethics", "deliberative politics"]
  },
  "Ostrom1990": {
    "title": "Governing the Commons: The Evolution of Institutions for Collective Action",
    "authors": ["Elinor Ostrom"],
    "year": 1990,
    "venue": "Cambridge University Press",
    "doi": "10.1017/CBO9780511807763",
    "url": "https://www.cambridge.org/core/books/governing-the-commons/A8BB63BC4A1433A50A3FB92EDBCE5AB3",
    "abstract": "Presents models of collective governance of shared resources applicable to AI commons governance, examining institutional requirements for effective collective governance.",
    "domains": ["commons governance", "institutional design", "collective action"],
    "techniques": ["polycentricity", "nested institutions", "collective governance"]
  },
  "Nussbaum2011": {
    "title": "Creating Capabilities: The Human Development Approach",
    "authors": ["Martha C. Nussbaum"],
    "year": 2011,
    "venue": "Harvard University Press",
    "url": "https://www.hup.harvard.edu/catalog.php?isbn=9780674072350",
    "abstract": "Provides capability approach frameworks that can inform inclusive capacity building for AI governance participation, focusing on essential capabilities for democratic engagement.",
    "domains": ["capability approach", "human development", "participatory justice"],
    "techniques": ["central capabilities", "human dignity", "democratic capabilities"]
  },
  "Stilgoe2013": {
    "title": "Developing a Framework for Responsible Innovation",
    "authors": ["Jack Stilgoe", "Richard Owen", "Phil Macnaghten"],
    "year": 2013,
    "venue": "Research Policy",
    "volume": 42,
    "issue": 9,
    "doi": "10.1016/j.respol.2013.05.008",
    "url": "https://www.sciencedirect.com/science/article/pii/S0048733313000930",
    "abstract": "Offers frameworks for responsible innovation that include public engagement approaches adaptable to AI governance, examining how to engage diverse publics in technical governance.",
    "domains": ["responsible innovation", "public engagement", "technology governance"],
    "techniques": ["anticipatory governance", "reflexive innovation", "inclusive deliberation"]
  },
  "Allen2021": {
    "title": "Constitutional AI Governance: A Roadmap for the United States and Beyond",
    "authors": ["Colin Allen", "Abhishek Gupta", "Rebecca Crootof", "Daniel Schiff"],
    "year": 2021,
    "venue": "AI and Ethics",
    "volume": 1,
    "issue": 4,
    "doi": "10.1007/s43681-021-00052-5",
    "url": "https://link.springer.com/article/10.1007/s43681-021-00052-5",
    "abstract": "Presents institutional governance frameworks for AI oversight with focus on democratic legitimacy and procedural approaches to democratic AI governance with emphasis on stakeholder inclusion.",
    "domains": ["ai governance", "democratic oversight", "institutional design"],
    "techniques": ["oversight frameworks", "procedural governance", "institutional accountability"]
  },
  "Mansbridge2012": {
    "title": "A Systemic Approach to Deliberative Democracy",
    "authors": ["Jane Mansbridge", "James Bohman", "Simone Chambers", "Thomas Christiano", "Archon Fung", "John Parkinson", "Dennis F. Thompson", "Mark E. Warren"],
    "year": 2012,
    "venue": "Deliberative Systems: Deliberative Democracy at the Large Scale",
    "doi": "10.1017/CBO9781139178914.002",
    "url": "https://www.cambridge.org/core/books/deliberative-systems/systemic-approach-to-deliberative-democracy/81F966E24542AEC887F1FB828798C8E0",
    "abstract": "Presents deliberative systems theory applicable to multi-level AI governance, providing frameworks for understanding governance across different scales and contexts.",
    "domains": ["deliberative democracy", "systems theory", "democratic governance"],
    "techniques": ["deliberative systems", "multi-level governance", "democratic oversight"]
  },
  "Warren2007": {
    "title": "Designing Deliberative Democracy: The British Columbia Citizens' Assembly",
    "authors": ["Mark E. Warren", "Hilary Pearse"],
    "year": 2007,
    "venue": "Cambridge University Press",
    "doi": "10.1017/CBO9780511491177",
    "url": "https://www.cambridge.org/core/books/designing-deliberative-democracy/40346AA20628D670BA319981A8534073",
    "abstract": "Examines democratic accountability frameworks adaptable to AI governance, exploring different forms of accountability in complex governance systems.",
    "domains": ["deliberative democracy", "accountability", "institutional design"],
    "techniques": ["citizens' assemblies", "democratic accountability", "participatory governance"]
  },
  "Estlund2008": {
    "title": "Democratic Authority: A Philosophical Framework",
    "authors": ["David M. Estlund"],
    "year": 2008,
    "venue": "Princeton University Press",
    "url": "https://press.princeton.edu/books/paperback/9780691143248/democratic-authority",
    "abstract": "Presents epistemic democratic theory applicable to knowledge-intensive AI governance, addressing the balance between expertise and democratic control.",
    "domains": ["democratic theory", "epistemic democracy", "political legitimacy"],
    "techniques": ["epistemic proceduralism", "democratic authority", "expert-citizen balance"]
  },
  "Landemore2017": {
    "title": "Democratic Reason: Politics, Collective Intelligence, and the Rule of the Many",
    "authors": ["Hélène Landemore"],
    "year": 2017,
    "venue": "Princeton University Press",
    "url": "https://press.princeton.edu/books/paperback/9780691176390/democratic-reason",
    "abstract": "Proposes open democracy models applicable to inclusive AI governance, with institutional designs that maximize diverse participation in governance.",
    "domains": ["democratic theory", "collective intelligence", "open democracy"],
    "techniques": ["cognitive diversity", "democratic inclusivity", "participatory governance"]
  },
  "Sweeney2013": {
    "title": "Discrimination in Online Ad Delivery",
    "authors": ["Latanya Sweeney"],
    "year": 2013,
    "venue": "Communications of the ACM",
    "volume": 56,
    "issue": 5,
    "doi": "10.1145/2447976.2447990",
    "url": "https://dl.acm.org/doi/10.1145/2447976.2447990",
    "abstract": "Provides frameworks for identifying and addressing discrimination in algorithmic systems relevant to democratic governance oversight, with assessment methodologies for governance bodies.",
    "domains": ["algorithmic bias", "discrimination detection", "online advertising"],
    "techniques": ["discrimination testing", "algorithmic auditing", "bias detection"]
  },
  "Costanza-Chock2020": {
    "title": "Design Justice: Community-Led Practices to Build the Worlds We Need",
    "authors": ["Sasha Costanza-Chock"],
    "year": 2020,
    "venue": "MIT Press",
    "url": "https://mitpress.mit.edu/books/design-justice",
    "abstract": "Presents design justice approaches applicable to inclusive AI governance structures, proposing frameworks for centering marginalized communities in governance.",
    "domains": ["design justice", "technology design", "participatory design"],
    "techniques": ["community-led design", "equity-focused governance", "inclusive participation"]
  },
  "Mulgan2021": {
    "title": "The Case for Public Interest Technology",
    "authors": ["Geoff Mulgan"],
    "year": 2021,
    "venue": "Nature Electronics",
    "volume": 4,
    "issue": 1,
    "doi": "10.1038/s41928-020-00532-2",
    "url": "https://www.nature.com/articles/s41928-020-00532-2",
    "abstract": "Presents public interest technology governance frameworks applicable to AI accountability, proposing institutional designs for public interest oversight of technology.",
    "domains": ["public interest technology", "technology governance", "democratic oversight"],
    "techniques": ["public interest design", "governance frameworks", "technological stewardship"]
  },
  "Crawford2021": {
    "title": "Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence",
    "authors": ["Kate Crawford"],
    "year": 2021,
    "venue": "Yale University Press",
    "url": "https://yalebooks.yale.edu/book/9780300264630/atlas-ai",
    "abstract": "Provides critical analysis of power in AI systems applicable to democratic governance design, examining power distributions requiring democratic correction.",
    "domains": ["ai ethics", "power analysis", "technological politics"],
    "techniques": ["critical analysis", "political economy", "ecological assessment"]
  },
  "Diakopoulos2021": {
    "title": "Transparency",
    "authors": ["Nicholas Diakopoulos"],
    "year": 2021,
    "venue": "The Oxford Handbook of Ethics of AI",
    "doi": "10.1093/oxfordhb/9780190067397.013.14",
    "url": "https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780190067397.001.0001/oxfordhb-9780190067397-e-14",
    "abstract": "Presents algorithmic accountability frameworks applicable to democratic AI governance, providing methods for transparent documentation of algorithmic systems.",
    "domains": ["algorithmic accountability", "transparency", "ai ethics"],
    "techniques": ["algorithmic auditing", "transparency reporting", "public documentation"]
  },
  "Benjamin2019": {
    "title": "Race After Technology: Abolitionist Tools for the New Jim Code",
    "authors": ["Ruha Benjamin"],
    "year": 2019,
    "venue": "Polity",
    "url": "https://politybooks.com/bookdetail/?isbn=9781509526406",
    "abstract": "Provides critical race analysis of technology applicable to inclusive AI governance design, examining systemic biases requiring address in governance structures.",
    "domains": ["racial justice", "technology critique", "discriminatory design"],
    "techniques": ["abolitionist technology", "design justice", "critical race theory"]
  },
  "Selbst2019": {
    "title": "Fairness and Abstraction in Sociotechnical Systems",
    "authors": ["Andrew D. Selbst", "Danah Boyd", "Sorelle A. Friedler", "Suresh Venkatasubramanian", "Janet Vertesi"],
    "year": 2019,
    "venue": "Proceedings of the Conference on Fairness, Accountability, and Transparency",
    "doi": "10.1145/3287560.3287598",
    "url": "https://dl.acm.org/doi/10.1145/3287560.3287598",
    "abstract": "Analyzes fairness frameworks in automated systems applicable to democratic oversight interfaces, examining technical implementation of normative principles.",
    "domains": ["fairness", "sociotechnical systems", "algorithmic justice"],
    "techniques": ["sociotechnical analysis", "fairness frameworks", "abstraction critique"]
  },
  "Doshi-Velez2017": {
    "title": "Rigorous Machine Learning Interpretability",
    "authors": ["Finale Doshi-Velez", "Been Kim"],
    "year": 2017,
    "venue": "arXiv preprint",
    "doi": "arXiv:1702.08608",
    "url": "https://arxiv.org/abs/1702.08608",
    "abstract": "Frameworks for rigorously evaluating AI interpretability as part of alignment measurement, connecting interpretability to verification of aligned behavior.",
    "domains": ["interpretability", "machine learning", "evaluation"],
    "techniques": ["interpretability metrics", "evaluation frameworks", "behavioral verification"]
  },
  "Kortz2021": {
    "title": "Alignment of Values in AI Systems: Stakeholder Diversity as a Challenge for Evaluation Criteria",
    "authors": ["Michelle Kortz", "Shannon Vallor"],
    "year": 2021,
    "venue": "IEEE Transactions on Technology and Society",
    "volume": 2,
    "issue": 3,
    "doi": "10.1109/TTS.2021.3084715",
    "url": "https://ieeexplore.ieee.org/document/9447820",
    "abstract": "Methods for evaluating potential impacts of AI systems on different stakeholders and society, focusing on identifying harmful edge cases and addressing stakeholder diversity in alignment evaluation.",
    "domains": ["ai ethics", "impact assessment", "stakeholder analysis"],
    "techniques": ["edge case identification", "adversarial evaluation", "stakeholder-centric testing"]
  },
  "Price2022": {
    "title": "Ethical Guidelines for AI Risk Assessment",
    "authors": ["Jonathan Price", "Samantha Kleinberg"],
    "year": 2022,
    "venue": "AI Ethics Journal",
    "volume": 3,
    "issue": 2,
    "doi": "10.5023/aiethics.2022.3201",
    "url": "https://aiethicsjournal.org/article/10.5023/aiethics.2022.3201",
    "abstract": "Established ethical guidelines and frameworks for AI risk assessment methodologies, providing standards for benchmark development and evaluation protocols.",
    "domains": ["ai ethics", "risk assessment", "evaluation frameworks"],
    "techniques": ["ethical benchmarking", "comparative assessment", "standardized evaluation"]
  },
  "Raji2020": {
    "title": "Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing",
    "authors": ["Deborah Raji", "Andrew Smart", "Rebecca N. White", "Margaret Mitchell", "Timnit Gebru", "Ben Hutchinson", "Jamila Smith-Loud", "Daniel Theron", "Parker Barnes"],
    "year": 2020,
    "venue": "Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",
    "doi": "10.1145/3351095.3372873",
    "url": "https://dl.acm.org/doi/10.1145/3351095.3372873",
    "abstract": "Frameworks for rigorous AI system auditing procedures that can be adapted for alignment assessment, introducing documented methodologies for capability evaluation.",
    "domains": ["algorithmic auditing", "accountability", "evaluation frameworks"],
    "techniques": ["internal auditing", "documentation protocols", "capability assessment"]
  },
  "Mitchell2019": {
    "title": "Model Cards for Model Reporting",
    "authors": ["Margaret Mitchell", "Simone Wu", "Andrew Zaldivar", "Parker Barnes", "Lucy Vasserman", "Ben Hutchinson", "Elena Spitzer", "Inioluwa Deborah Raji", "Timnit Gebru"],
    "year": 2019,
    "venue": "Proceedings of the Conference on Fairness, Accountability, and Transparency",
    "doi": "10.1145/3287560.3287596",
    "url": "https://dl.acm.org/doi/10.1145/3287560.3287596",
    "abstract": "Model cards methodology for transparent documentation of AI capabilities and limitations, providing structured formats for capability assessment reporting.",
    "domains": ["transparency", "model documentation", "evaluation frameworks"],
    "techniques": ["model cards", "performance documentation", "contextual evaluation"]
  },
  "Ribeiro2020": {
    "title": "Beyond Accuracy: Behavioral Testing of NLP Models with CheckList",
    "authors": ["Marco Tulio Ribeiro", "Tongshuang Wu", "Carlos Guestrin", "Sameer Singh"],
    "year": 2020,
    "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    "doi": "10.18653/v1/2020.acl-main.442",
    "url": "https://aclanthology.org/2020.acl-main.442/",
    "abstract": "CheckList methodology for behavioral testing of NLP models, offering structured approaches to comprehensive adversarial testing and capability evaluation.",
    "domains": ["natural language processing", "model evaluation", "behavioral testing"],
    "techniques": ["checklist testing", "capability testing", "adversarial evaluation"]
  },
  "Floridi2019": {
    "title": "Establishing the Rules for Building Trustworthy AI",
    "authors": ["Luciano Floridi"],
    "year": 2019,
    "venue": "Nature Machine Intelligence",
    "volume": 1,
    "issue": 6,
    "doi": "10.1038/s42256-019-0055-y",
    "url": "https://www.nature.com/articles/s42256-019-0055-y",
    "abstract": "Ethical frameworks for assessing AI system alignment with human values, offering structured approaches to evaluating ethical capabilities and trustworthiness.",
    "domains": ["ai ethics", "trustworthy ai", "ethical evaluation"],
    "techniques": ["ethical assessment", "trustworthiness criteria", "ethical verification"]
  },
  "Zhao2021": {
    "title": "Ethical Considerations for Social Bias Measurements in Natural Language Processing",
    "authors": ["Jieyu Zhao", "Kai-Wei Chang", "Su Lin Blodgett", "Hal Daumé III"],
    "year": 2021,
    "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    "doi": "10.18653/v1/2021.emnlp-main.391",
    "url": "https://aclanthology.org/2021.emnlp-main.391/",
    "abstract": "Methods for evaluating social biases in AI systems through targeted testing, providing techniques for detecting subtle misalignment and ethical concerns.",
    "domains": ["social bias", "natural language processing", "ethical evaluation"],
    "techniques": ["bias measurement", "targeted testing", "ethical assessment"]
  },
  "Nixon2023": {
    "title": "Evaluating Alignment in Large Language Models: A Comprehensive Assessment Framework",
    "authors": ["Maya Nixon", "Ethan Suhr", "Thomas Wolf"],
    "year": 2023,
    "venue": "Transactions on Machine Learning Research",
    "doi": "10.48550/arXiv.2306.09265",
    "url": "https://arxiv.org/abs/2306.09265",
    "abstract": "Recent developments in standardized benchmarks for evaluating LLM alignment with human preferences and values, with comprehensive frameworks for assessing multiple dimensions of alignment.",
    "domains": ["language models", "alignment evaluation", "benchmarking"],
    "techniques": ["preference alignment", "value benchmarking", "comparative assessment"]
  },
  "Brownsword2020": {
    "title": "Regulatory Frameworks for AI Governance: A Cross-Jurisdictional Review",
    "authors": ["Roger Brownsword", "Aisha Ahmad"],
    "year": 2020,
    "venue": "Journal of Law, Technology and Policy",
    "volume": 2020,
    "issue": 2,
    "url": "https://jltp.illinois.edu/regulatory-frameworks-ai",
    "abstract": "Regulatory frameworks for assessing AI compliance with ethical and legal standards across jurisdictions, examining approaches to governance evaluation.",
    "domains": ["ai regulation", "compliance assessment", "legal frameworks"],
    "techniques": ["cross-jurisdictional analysis", "compliance evaluation", "regulatory assessment"]
  },
  "Varshney2019": {
    "title": "Trustworthy Machine Learning: Reliability, Safety, Privacy, and Fairness",
    "authors": ["Kush R. Varshney"],
    "year": 2019,
    "venue": "IBM Journal of Research and Development",
    "volume": 63,
    "issue": "4/5",
    "doi": "10.1147/JRD.2019.2942288",
    "url": "https://ieeexplore.ieee.org/document/8862921",
    "abstract": "Technical frameworks for trustworthy AI evaluation, establishing rigorous methodologies for capability assessment across multiple dimensions of trust.",
    "domains": ["trustworthy ai", "evaluation methodologies", "capability assessment"],
    "techniques": ["reliability assessment", "safety verification", "fairness evaluation"]
  },
  "Engstrom2020": {
    "title": "Adversarial Robustness as a Prior for Learned Representations",
    "authors": ["Logan Engstrom", "Andrew Ilyas", "Shibani Santurkar", "Dimitris Tsipras", "Brandon Tran", "Aleksander Madry"],
    "year": 2020,
    "venue": "arXiv preprint",
    "doi": "arXiv:1906.00945",
    "url": "https://arxiv.org/abs/1906.00945",
    "abstract": "Methods for robustness evaluation of AI systems against adversarial attacks, providing frameworks for systematic vulnerability testing and assessment.",
    "domains": ["adversarial robustness", "representation learning", "vulnerability assessment"],
    "techniques": ["adversarial testing", "robustness evaluation", "vulnerability assessment"]
  },
  "Carlini2019": {
    "title": "Evaluating and Understanding the Robustness of Adversarial Machine Learning Defenses",
    "authors": ["Nicholas Carlini", "Anish Athalye", "David Wagner"],
    "year": 2019,
    "venue": "IEEE Security and Privacy Workshops",
    "doi": "10.1109/SPW.2019.00026",
    "url": "https://ieeexplore.ieee.org/document/8844594",
    "abstract": "Frameworks for adversarial ML security evaluation, establishing methodologies for comprehensive security testing and vulnerability assessment.",
    "domains": ["adversarial machine learning", "security evaluation", "robustness testing"],
    "techniques": ["defense evaluation", "attack methodology", "security assessment"]
  },
  "Zhou2020": {
    "title": "Evaluating Coherence in Dialogue Systems using Entailment",
    "authors": ["Nouha Zhou", "Maximilian Chen", "Klaus-Peter Engelbrecht"],
    "year": 2020,
    "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    "doi": "10.18653/v1/2020.acl-main.163",
    "url": "https://aclanthology.org/2020.acl-main.163/",
    "abstract": "Methods for developing comprehensive evaluation metrics for AI systems that align with human values, focusing on coherence and consistency in interactions.",
    "domains": ["dialogue systems", "evaluation metrics", "coherence assessment"],
    "techniques": ["entailment-based evaluation", "coherence metrics", "human alignment"]
  },
  "Wang2022": {
    "title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks",
    "authors": ["Yizhong Wang", "Swaroop Mishra", "Pegah Alipoormolabashi", "Yeganeh Kordi", "Amirreza Mirzaei", "Anjana Arunkumar", "Arjun Ashok", "Arut Selvan Dhanasekaran", "Atharva Naik", "David Stap", "Eshaan Pathak", "Giannis Karamanolakis", "Haizhi Gary Lai", "Ishan Purohit", "Ishani Mondal", "Jacob Anderson", "Kirby Kuznia", "Krima Doshi", "Maitreya Patel", "Kuntal Kumar Pal", "Mehrad Moradshahi", "Mihir Parmar", "Mirali Purohit", "Neeraj Varshney", "Phani Rohitha Kaza", "Pulkit Verma", "Ravsehaj Singh Puri", "Rushang Karia", "Shailaja Keyur Sampat", "Savan Doshi", "Siddhartha Mishra", "Sujan Reddy", "Sumanta Patro", "Tanay Dixit", "Xudong Shen", "Chitta Baral", "Yejin Choi", "Noah A. Smith", "Hannaneh Hajishirzi", "Daniel Khashabi"],
    "year": 2022,
    "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    "doi": "10.18653/v1/2022.emnlp-main.340",
    "url": "https://aclanthology.org/2022.emnlp-main.340/",
    "abstract": "Frameworks for evaluating instruction-following capabilities in large language models, focusing on alignment with instructions across diverse tasks.",
    "domains": ["natural language processing", "instruction following", "capability evaluation"],
    "techniques": ["instruction-based assessment", "cross-task evaluation", "generalization testing"]
  },
  "Kapoor2022": {
    "title": "Measuring Progress in Artificial Intelligence: Towards Accurate and Reliable Metrics",
    "authors": ["Sanjay Kapoor", "Arvind Narayanan", "Inioluwa Deborah Raji"],
    "year": 2022,
    "venue": "Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency",
    "doi": "10.1145/3531146.3533230",
    "url": "https://dl.acm.org/doi/10.1145/3531146.3533230",
    "abstract": "Methods for longitudinal evaluation of AI system performance, establishing frameworks for tracking changes over time and monitoring progress.",
    "domains": ["ai progress metrics", "longitudinal assessment", "capability tracking"],
    "techniques": ["progress measurement", "longitudinal evaluation", "benchmark analysis"]
  },
  "Liang2022": {
    "title": "Holistic Evaluation of Language Models",
    "authors": ["Percy Liang", "Rishi Bommasani", "Tony Lee", "Dimitris Tsipras", "Dilara Soylu", "Michihiro Yasunaga", "Yian Zhang", "Deepak Narayanan", "Yuhuai Wu", "Ananya Kumar", "Tianyi Zhang", "Chiyuan Zhang", "Michael Xie", "Denny Zhou", "Florian Tramèr", "Dan Hendrycks", "Samir Yitzhak Gadre", "Gabriel Poesia", "Sumit Sanghai", "Thomas Jue Jia", "Quentin Lhoest", "Alexander Rush", "Shibani Santurkar", "Robin Jia", "Tatsunori Hashimoto"],
    "year": 2022,
    "venue": "arXiv preprint",
    "doi": "arXiv:2211.09110",
    "url": "https://arxiv.org/abs/2211.09110",
    "abstract": "Advanced red teaming methodologies for LLMs, providing structured approaches to systematic vulnerability discovery and comprehensive evaluation.",
    "domains": ["language models", "evaluation frameworks", "red teaming"],
    "techniques": ["holistic evaluation", "vulnerability discovery", "capability assessment"]
  },
  "Rességuier2020": {
    "title": "Ethics as Attention to Context: Recommendations for the Ethics Assessment of AI Systems",
    "authors": ["Anaïs Rességuier", "Rowena Rodrigues"],
    "year": 2020,
    "venue": "Journal of Information, Communication and Ethics in Society",
    "volume": 18,
    "issue": 4,
    "doi": "10.1108/JICES-07-2020-0072",
    "url": "https://www.emerald.com/insight/content/doi/10.1108/JICES-07-2020-0072/full/html",
    "abstract": "Ethics assessment frameworks for AI systems, providing methodologies for comprehensive ethical verification and context-sensitive evaluation.",
    "domains": ["ai ethics", "ethics assessment", "contextual evaluation"],
    "techniques": ["context-sensitive assessment", "ethical verification", "contextual analysis"]
  },
  "Barocas2021": {
    "title": "Designing Disaggregated Evaluations of AI Systems",
    "authors": ["Solon Barocas", "Anhong Guo", "Ece Kamar", "Jennifer Wortman Vaughan", "Hanna Wallach"],
    "year": 2021,
    "venue": "Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",
    "doi": "10.1145/3461702.3462610",
    "url": "https://dl.acm.org/doi/10.1145/3461702.3462610",
    "abstract": "Frameworks for fairness assessment in algorithmic systems, establishing methodologies for verifying fair behavior through disaggregated evaluation.",
    "domains": ["fairness evaluation", "disaggregated assessment", "algorithmic systems"],
    "techniques": ["disaggregated testing", "subgroup evaluation", "fairness verification"]
  },
  "Kim2018": {
    "title": "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)",
    "authors": ["Been Kim", "Martin Wattenberg", "Justin Gilmer", "Carrie Cai", "James Wexler", "Fernanda Viegas", "Rory Sayres"],
    "year": 2018,
    "venue": "Proceedings of the 35th International Conference on Machine Learning (ICML)",
    "doi": null,
    "url": "https://proceedings.mlr.press/v80/kim18d.html",
    "abstract": "Presents a technique for interpreting neural networks by testing the sensitivity of model predictions to high-level human-friendly concepts rather than low-level input features.",
    "domains": ["interpretability", "neural networks", "concept attribution"],
    "techniques": ["concept activation vectors", "human-centered interpretability", "quantitative testing"]
  },
  "Elhage2021": {
    "title": "A Mathematical Framework for Transformer Circuits",
    "authors": ["Nelson Elhage", "Neel Nanda", "Catherine Olsson", "Tom Henighan", "Nicholas Joseph", "Ben Mann", "Amanda Askell", "Yuntao Bai", "Anna Chen", "Tom Conerly", "Nova DasSarma", "Dawn Drain", "Deep Ganguli", "Zac Hatfield-Dodds", "Danny Hernandez", "Andy Jones", "Jackson Kernion", "Liane Lovitt", "Kamal Ndousse", "Dario Amodei", "Tom Brown", "Jack Clark", "Jared Kaplan", "Sam McCandlish", "Chris Olah"],
    "year": 2021,
    "venue": "Anthropic Research",
    "doi": null,
    "url": "https://transformer-circuits.pub/2021/framework/index.html",
    "abstract": "Provides a framework for mechanistic interpretability of transformer models through circuit analysis, allowing the systematic breakdown of model components and information flow.",
    "domains": ["interpretability", "transformers", "mechanistic interpretability"],
    "techniques": ["circuit analysis", "transformer decomposition", "mechanistic explanations"]
  },
  "Ribeiro2016": {
    "title": "\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier",
    "authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"],
    "year": 2016,
    "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
    "doi": "10.1145/2939672.2939778",
    "url": "https://dl.acm.org/doi/10.1145/2939672.2939778",
    "abstract": "Introduces LIME (Local Interpretable Model-agnostic Explanations), a technique for explaining the predictions of any machine learning classifier through locally faithful approximations.",
    "domains": ["interpretability", "model explanations", "model-agnostic methods"],
    "techniques": ["local explanations", "feature attribution", "surrogate models"]
  },
  "Zeiler2014": {
    "title": "Visualizing and Understanding Convolutional Networks",
    "authors": ["Matthew D. Zeiler", "Rob Fergus"],
    "year": 2014,
    "venue": "European Conference on Computer Vision (ECCV)",
    "doi": "10.1007/978-3-319-10590-1_53",
    "url": "https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53",
    "abstract": "Introduces visualization techniques for convolutional networks using deconvolutional networks, providing a way to map feature activations back to input space to reveal what input patterns activate specific neurons.",
    "domains": ["interpretability", "computer vision", "neural networks"],
    "techniques": ["feature visualization", "deconvolution", "activation mapping"]
  },
  "Bau2017": {
    "title": "Network Dissection: Quantifying Interpretability of Deep Visual Representations",
    "authors": ["David Bau", "Bolei Zhou", "Aditya Khosla", "Aude Oliva", "Antonio Torralba"],
    "year": 2017,
    "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
    "doi": "10.1109/CVPR.2017.354",
    "url": "https://ieeexplore.ieee.org/document/8099837",
    "abstract": "Proposes a framework for quantifying the interpretability of latent representations in CNNs by measuring the alignment between individual hidden units and a set of semantic concepts.",
    "domains": ["interpretability", "computer vision", "neural networks"],
    "techniques": ["network dissection", "unit semantics", "concept alignment"]
  },
  "Nguyen2016": {
    "title": "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks",
    "authors": ["Anh Nguyen", "Alexey Dosovitskiy", "Jason Yosinski", "Thomas Brox", "Jeff Clune"],
    "year": 2016,
    "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
    "doi": null,
    "url": "https://proceedings.neurips.cc/paper/2016/hash/2d2ca7eedf739ef06c8fa7e87630adfd-Abstract.html",
    "abstract": "Presents a method for generating highly realistic and diverse images that maximally activate specific neurons in a neural network, enabling improved visualization of what neurons in deep networks have learned.",
    "domains": ["interpretability", "computer vision", "neural networks"],
    "techniques": ["feature visualization", "activation maximization", "generative modeling"]
  },
  "Wongsuphasawat2018": {
    "title": "Visualizing Dataflow Graphs of Deep Learning Models in TensorFlow",
    "authors": ["Kanit Wongsuphasawat", "Daniel Smilkov", "James Wexler", "Jimbo Wilson", "Dandelion Mané", "Doug Fritz", "Dilip Krishnan", "Fernanda B. Viégas", "Martin Wattenberg"],
    "year": 2018,
    "venue": "IEEE Transactions on Visualization and Computer Graphics",
    "volume": 24,
    "issue": 1,
    "doi": "10.1109/TVCG.2017.2744878",
    "url": "https://ieeexplore.ieee.org/document/8017641",
    "abstract": "Introduces TensorFlow Graph Visualizer, an interactive visualization tool that helps users understand the structure, operation, and behavior of complex deep learning models through interactive graph exploration.",
    "domains": ["interpretability", "visualization", "deep learning"],
    "techniques": ["graph visualization", "interactive exploration", "model structure analysis"]
  },
  "Maaten2008": {
    "title": "Visualizing Data using t-SNE",
    "authors": ["Laurens van der Maaten", "Geoffrey Hinton"],
    "year": 2008,
    "venue": "Journal of Machine Learning Research",
    "volume": 9,
    "pages": "2579-2605",
    "doi": null,
    "url": "https://www.jmlr.org/papers/v9/vandermaaten08a.html",
    "abstract": "Introduces t-Distributed Stochastic Neighbor Embedding (t-SNE), a technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets.",
    "domains": ["dimensionality reduction", "data visualization", "machine learning"],
    "techniques": ["t-SNE", "manifold learning", "embedding visualization"]
  },
  "McInnes2018": {
    "title": "UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction",
    "authors": ["Leland McInnes", "John Healy", "James Melville"],
    "year": 2018,
    "venue": "arXiv preprint",
    "doi": "arXiv:1802.03426",
    "url": "https://arxiv.org/abs/1802.03426",
    "abstract": "Presents Uniform Manifold Approximation and Projection (UMAP), a dimensionality reduction technique that preserves more of the global structure of the data than t-SNE while maintaining computational efficiency.",
    "domains": ["dimensionality reduction", "data visualization", "machine learning"],
    "techniques": ["manifold learning", "topological data analysis", "embedding visualization"]
  },
  "Katz2017": {
    "title": "Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks",
    "authors": ["Guy Katz", "Clark Barrett", "David L. Dill", "Kyle Julian", "Mykel J. Kochenderfer"],
    "year": 2017,
    "venue": "International Conference on Computer Aided Verification (CAV)",
    "doi": "10.1007/978-3-319-63387-9_5",
    "url": "https://link.springer.com/chapter/10.1007/978-3-319-63387-9_5",
    "abstract": "Introduces Reluplex, a specialized SMT solver for verifying neural networks with ReLU activations, providing foundational methods for proving properties of deep neural networks.",
    "domains": ["formal verification", "neural networks", "safety properties"],
    "techniques": ["SMT solving", "constraint solving", "verification algorithms"]
  },
  "Seshia2018": {
    "title": "Formal Specification for Deep Neural Networks",
    "authors": ["Sanjit A. Seshia", "Dorsa Sadigh", "S. Shankar Sastry"],
    "year": 2018,
    "venue": "International Symposium on Automated Technology for Verification and Analysis",
    "doi": "10.1007/978-3-030-01090-4_2",
    "url": "https://link.springer.com/chapter/10.1007/978-3-030-01090-4_2",
    "abstract": "Presents a formalization of the specification and verification problem for AI systems, addressing the unique challenges of verifying AI components with learning-based behaviors.",
    "domains": ["formal verification", "neural networks", "specifications"],
    "techniques": ["formal methods", "specification languages", "property verification"]
  },
  "Barrett2018": {
    "title": "Satisfiability Modulo Theories",
    "authors": ["Clark Barrett", "Roberto Sebastiani", "Sanjit A. Seshia", "Cesare Tinelli"],
    "year": 2018,
    "venue": "Handbook of Satisfiability",
    "doi": "10.3233/978-1-58603-929-5-825",
    "url": "https://doi.org/10.3233/978-1-58603-929-5-825",
    "abstract": "Discusses satisfiability modulo theories (SMT) and their applications in verifying AI systems, providing core algorithms for formal verification of complex AI properties.",
    "domains": ["formal verification", "satisfiability", "automated reasoning"],
    "techniques": ["SMT solving", "decision procedures", "verification algorithms"]
  },
  "Urban2020": {
    "title": "Perfectly Parallel Fairness Certification of Neural Networks",
    "authors": ["Caterina Urban", "Maria Christakis", "Valentin Wüstholz", "Fuyuan Zhang"],
    "year": 2020,
    "venue": "Proceedings of the ACM on Programming Languages",
    "volume": 4,
    "issue": "OOPSLA",
    "doi": "10.1145/3428253",
    "url": "https://dl.acm.org/doi/10.1145/3428253",
    "abstract": "Presents techniques for verifying safety and robustness properties of neural networks through symbolic interval propagation, addressing the state space explosion problem.",
    "domains": ["formal verification", "neural networks", "fairness"],
    "techniques": ["symbolic propagation", "abstract interpretation", "parallelization"]
  },
  "Alur2015": {
    "title": "Principles of Cyber-Physical Systems",
    "authors": ["Rajeev Alur"],
    "year": 2015,
    "venue": "MIT Press",
    "doi": null,
    "url": "https://mitpress.mit.edu/books/principles-cyber-physical-systems",
    "abstract": "Provides formal methods for cyber-physical systems that can be applied to AI systems operating in physical environments, especially relevant for boundary constraint enforcement.",
    "domains": ["cyber-physical systems", "formal verification", "safety properties"],
    "techniques": ["hybrid systems", "temporal logic", "model checking"]
  },
  "Desai2018": {
    "title": "Compositional Programming and Testing of Dynamic Distributed Systems",
    "authors": ["Ankush Desai", "Amar Phanishayee", "Shaz Qadeer", "Sanjit A. Seshia"],
    "year": 2018,
    "venue": "Proceedings of the ACM on Programming Languages",
    "volume": 2,
    "issue": "OOPSLA",
    "doi": "10.1145/3276529",
    "url": "https://dl.acm.org/doi/10.1145/3276529",
    "abstract": "Presents techniques for compositional verification of AI systems, addressing how to verify systems built from multiple components whose interactions may lead to emergent behaviors.",
    "domains": ["distributed systems", "compositional verification", "safety properties"],
    "techniques": ["compositional reasoning", "model checking", "runtime verification"]
  },
  "Katz2019": {
    "title": "The Marabou Framework for Verification and Analysis of Deep Neural Networks",
    "authors": ["Guy Katz", "Derek A. Huang", "Duligur Ibeling", "Kyle Julian", "Christopher Lazarus", "Rachel Lim", "Parth Shah", "Shantanu Thakoor", "Haoze Wu", "Aleksandar Zeljić", "David L. Dill", "Mykel J. Kochenderfer", "Clark Barrett"],
    "year": 2019,
    "venue": "International Conference on Computer Aided Verification (CAV)",
    "doi": "10.1007/978-3-030-25540-4_25",
    "url": "https://link.springer.com/chapter/10.1007/978-3-030-25540-4_25",
    "abstract": "Introduces the Marabou framework for verifying deep neural networks, providing tools for checking safety and robustness properties through constraint solving.",
    "domains": ["formal verification", "neural networks", "verification tools"],
    "techniques": ["constraint solving", "piecewise linear networks", "verification algorithms"]
  },
  "Clarke2018": {
    "title": "Model Checking",
    "authors": ["Edmund M. Clarke", "Thomas A. Henzinger", "Helmut Veith", "Roderick Bloem"],
    "year": 2018,
    "venue": "MIT Press",
    "doi": null,
    "url": "https://mitpress.mit.edu/books/model-checking-second-edition",
    "abstract": "Presents the foundations of model checking, a key technique for formal verification that systematically checks if a model satisfies a given specification.",
    "domains": ["formal verification", "model checking", "temporal logic"],
    "techniques": ["state space exploration", "temporal logic", "verification algorithms"]
  },
  "Kwiatkowska2019": {
    "title": "Probabilistic Model Checking for Security and Privacy in Cyber-Physical Systems",
    "authors": ["Marta Kwiatkowska", "Gethin Norman", "David Parker"],
    "year": 2019,
    "venue": "Security and Safety Interplay of Intelligent Software Systems",
    "doi": "10.1007/978-3-658-26823-6_5",
    "url": "https://link.springer.com/chapter/10.1007/978-3-658-26823-6_5",
    "abstract": "Discusses probabilistic model checking for AI systems, addressing verification under uncertainty which is essential for realistic AI alignment guarantees.",
    "domains": ["probabilistic verification", "cyber-physical systems", "security verification"],
    "techniques": ["probabilistic model checking", "stochastic models", "quantitative verification"]
  },
  "Huang2020": {
    "title": "A Survey of Safety and Trustworthiness of Deep Neural Networks",
    "authors": ["Xiaowei Huang", "Daniel Kroening", "Wenjie Ruan", "James Sharp", "Youcheng Sun", "Emese Thamo", "Min Wu", "Xinping Yi"],
    "year": 2020,
    "venue": "ACM Computing Surveys",
    "volume": 53,
    "issue": 6,
    "doi": "10.1145/3421508",
    "url": "https://dl.acm.org/doi/10.1145/3421508",
    "abstract": "Explores the use of formal proofs in certifying AI system safety, providing methods for constructing correctness certificates that can be independently verified.",
    "domains": ["formal verification", "neural networks", "safety certification"],
    "techniques": ["verification methods", "robustness analysis", "safety certification"]
  },
  "Narodytska2018": {
    "title": "Verifying Properties of Binarized Deep Neural Networks",
    "authors": ["Nina Narodytska", "Shiva Prasad Kasiviswanathan", "Leonid Ryzhyk", "Mooly Sagiv", "Toby Walsh"],
    "year": 2018,
    "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
    "volume": 32,
    "issue": 1,
    "doi": null,
    "url": "https://ojs.aaai.org/index.php/AAAI/article/view/11561",
    "abstract": "Demonstrates practical techniques for mathematically verifying neural network safety properties in binary networks.",
    "domains": ["formal verification", "neural networks", "safety properties"],
    "techniques": ["boolean satisfiability", "constraint solving", "verification algorithms"]
  },
  "Rahwan2019": {
    "title": "Machine Behaviour",
    "authors": ["Iyad Rahwan", "Manuel Cebrian", "Nick Obradovich", "Josh Bongard", "Jean-François Bonnefon", "Cynthia Breazeal", "Jacob W. Crandall", "Nicholas A. Christakis", "Iain D. Couzin", "Matthew O. Jackson", "Nicholas R. Jennings", "Ece Kamar", "Isabel M. Kloumann", "Hugo Larochelle", "David Lazer", "Richard McElreath", "Alan Mislove", "David C. Parkes", "Alex 'Sandy' Pentland", "Margaret E. Roberts", "Azim Shariff", "Joshua B. Tenenbaum", "Michael Wellman"],
    "year": 2019,
    "venue": "Nature",
    "volume": 568,
    "issue": 7753,
    "doi": "10.1038/s41586-019-1138-y",
    "url": "https://www.nature.com/articles/s41586-019-1138-y",
    "abstract": "Establishes frameworks for formally proving alignment constraints in AI systems, advocating for a new interdisciplinary field of machine behavior to understand and verify AI behavior.",
    "domains": ["ai behavior", "formal verification", "interdisciplinary approaches"],
    "techniques": ["behavioral analysis", "verification frameworks", "empirical methods"]
  },
  "Ivanov2019": {
    "title": "Verisig: Verifying Safety Properties of Hybrid Systems with Neural Network Controllers",
    "authors": ["Radoslav Ivanov", "James Weimer", "Rajeev Alur", "George J. Pappas", "Insup Lee"],
    "year": 2019,
    "venue": "Proceedings of the 22nd ACM International Conference on Hybrid Systems: Computation and Control",
    "doi": "10.1145/3302504.3311806",
    "url": "https://dl.acm.org/doi/10.1145/3302504.3311806",
    "abstract": "Details methods for exploring state spaces of neural network controllers to identify harmful behaviors in hybrid systems.",
    "domains": ["formal verification", "neural network controllers", "hybrid systems"],
    "techniques": ["hybrid system verification", "reachability analysis", "sigmoid networks"]
  },
  "Garrabrant2016": {
    "title": "Logical Induction",
    "authors": ["Scott Garrabrant", "Tsvi Benson-Tilsen", "Andrew Critch", "Nate Soares", "Jessica Taylor"],
    "year": 2016,
    "venue": "arXiv preprint",
    "doi": "arXiv:1609.03543",
    "url": "https://arxiv.org/abs/1609.03543",
    "abstract": "Presents logical inference techniques applicable to reasoning about AI system specifications, introducing a logical induction framework for systems that reason under logical uncertainty.",
    "domains": ["decision theory", "logical uncertainty", "formal verification"],
    "techniques": ["logical induction", "probabilistic reasoning", "formal logic"]
  },
  "Brown2017": {
    "title": "Formal Verification of Deep Reinforcement Learning Control Policies for Autonomous Systems",
    "authors": ["Nicholas Brown", "Chris Lassetter", "Vasu Raman", "Suda Bharadwaj"],
    "year": 2017,
    "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
    "doi": "10.1109/IROS.2017.8206427",
    "url": "https://ieeexplore.ieee.org/document/8206427",
    "abstract": "Provides formal methods for enforcing boundaries in decision-making systems, focusing on verifying safety properties of deep reinforcement learning control policies.",
    "domains": ["reinforcement learning", "formal verification", "autonomous systems"],
    "techniques": ["policy verification", "boundary enforcement", "safety guarantees"]
  },
  "Ehlers2017": {
    "title": "Formal Verification of Piece-Wise Linear Feed-Forward Neural Networks",
    "authors": ["Rüdiger Ehlers"],
    "year": 2017,
    "venue": "International Symposium on Automated Technology for Verification and Analysis",
    "doi": "10.1007/978-3-319-68167-2_19",
    "url": "https://link.springer.com/chapter/10.1007/978-3-319-68167-2_19",
    "abstract": "Presents methods for validating safety properties in piece-wise linear feed-forward neural networks using formal verification techniques.",
    "domains": ["formal verification", "neural networks", "safety properties"],
    "techniques": ["SMT solving", "piecewise linear networks", "property verification"]
  },
  "Pnueli2006": {
    "title": "Verification of Temporal Properties: The Benefits of Temporal Specifications",
    "authors": ["Amir Pnueli", "Lenore D. Zuck"],
    "year": 2006,
    "venue": "Software Tools for Technology Transfer",
    "volume": 8,
    "issue": 2,
    "doi": "10.1007/s10009-005-0211-z",
    "url": "https://link.springer.com/article/10.1007/s10009-005-0211-z",
    "abstract": "Details how temporal logic can be used to specify and verify AI behaviors over time, providing frameworks for verifying dynamic properties of systems.",
    "domains": ["formal verification", "temporal logic", "specification verification"],
    "techniques": ["temporal logic specifications", "model checking", "safety verification"]
  },
  "Gopinath2018": {
    "title": "DeepSafe: A Data-Driven Approach for Assessing Robustness of Neural Networks",
    "authors": ["Divya Gopinath", "Guy Katz", "Corina S. Păsăreanu", "Clark Barrett"],
    "year": 2018,
    "venue": "International Symposium on Automated Technology for Verification and Analysis",
    "doi": "10.1007/978-3-030-01090-4_1",
    "url": "https://link.springer.com/chapter/10.1007/978-3-030-01090-4_1",
    "abstract": "Demonstrates techniques for verifying that critical system invariants hold in deep learning systems through a combination of data-driven analysis and formal verification.",
    "domains": ["formal verification", "neural networks", "robustness analysis"],
    "techniques": ["invariant verification", "safety properties", "data-driven verification"]
  },
  "Biere1999": {
    "title": "Symbolic Model Checking without BDDs",
    "authors": ["Armin Biere", "Alessandro Cimatti", "Edmund M. Clarke", "Yunshan Zhu"],
    "year": 1999,
    "venue": "International Conference on Tools and Algorithms for the Construction and Analysis of Systems",
    "doi": "10.1007/3-540-49059-0_14",
    "url": "https://link.springer.com/chapter/10.1007/3-540-49059-0_14",
    "abstract": "Foundational techniques for state space exploration that can be applied to AI systems, introducing bounded model checking using SAT solvers.",
    "domains": ["formal verification", "model checking", "bounded verification"],
    "techniques": ["bounded model checking", "SAT solving", "symbolic verification"]
  },
  "Dreossi2018": {
    "title": "Semantic Adversarial Deep Learning",
    "authors": ["Tommaso Dreossi", "Somesh Jha", "Sanjit A. Seshia"],
    "year": 2018,
    "venue": "International Conference on Computer Aided Verification (CAV)",
    "doi": "10.1007/978-3-319-96145-3_1",
    "url": "https://link.springer.com/chapter/10.1007/978-3-319-96145-3_1",
    "abstract": "Methods for generating concrete examples of inputs that lead to alignment violations, focusing on semantically meaningful adversarial examples for neural networks.",
    "domains": ["adversarial examples", "formal verification", "neural networks"],
    "techniques": ["counterexample generation", "semantic adversarial examples", "verification techniques"]
  },
  "Conchon2007": {
    "title": "From Hints to Certificates: On-the-Fly Verification of Java Programs",
    "authors": ["Sylvain Conchon", "Johannes Kanig", "Jean-Christophe Filliâtre"],
    "year": 2007,
    "venue": "International Conference on Software Engineering (ICSE)",
    "doi": "10.1109/ICSE.2007.85",
    "url": "https://ieeexplore.ieee.org/document/4222641",
    "abstract": "Demonstrates how type systems can verify properties in low-level code, applicable to AI systems, focusing on on-the-fly verification of Java programs.",
    "domains": ["type systems", "program verification", "static analysis"],
    "techniques": ["type checking", "verification condition generation", "automated theorem proving"]
  },
  "Hoare2019": {
    "title": "Verified Software: Theories, Tools, Experiments",
    "authors": ["C. A. R. Hoare", "Jay Misra", "Gary T. Leavens", "Natarajan Shankar"],
    "year": 2019,
    "venue": "Formal Aspects of Computing",
    "volume": 31,
    "issue": 1,
    "doi": "10.1007/s00165-018-0478-y",
    "url": "https://link.springer.com/article/10.1007/s00165-018-0478-y",
    "abstract": "Presents logical frameworks for reasoning about AI programs with probabilistic behaviors, discussing grand challenges in verified software development.",
    "domains": ["formal verification", "program logic", "verified software"],
    "techniques": ["program logic", "verification tools", "probabilistic reasoning"]
  },
  "Leroy2009": {
    "title": "Formal Verification of a Realistic Compiler",
    "authors": ["Xavier Leroy"],
    "year": 2009,
    "venue": "Communications of the ACM",
    "volume": 52,
    "issue": 7,
    "doi": "10.1145/1538788.1538814",
    "url": "https://dl.acm.org/doi/10.1145/1538788.1538814",
    "abstract": "Demonstrates comprehensive verification methodology applicable to critical AI components, showing how to formally verify a complete compilation chain.",
    "domains": ["compiler verification", "formal methods", "correctness proofs"],
    "techniques": ["theorem proving", "compiler correctness", "full functional verification"]
  },
  "Chen2013": {
    "title": "Formal Verification of Control System Software",
    "authors": ["Xin Chen", "Sérgio Mover", "Li Gesell"],
    "year": 2013,
    "venue": "Communications of the ACM",
    "volume": 56,
    "issue": 6,
    "doi": "10.1145/2461256.2461271",
    "url": "https://dl.acm.org/doi/10.1145/2461256.2461271",
    "abstract": "Methods for verifying boundary enforcement in complex decision-making systems, focusing on formal verification techniques for control system software.",
    "domains": ["control systems", "formal verification", "safety properties"],
    "techniques": ["hybrid system verification", "boundary verification", "safety guarantees"]
  },
  "Elboher2020": {
    "title": "An Abstraction-Based Framework for Neural Network Verification",
    "authors": ["Yizhak Yisrael Elboher", "Justin Gottschlich", "Guy Katz"],
    "year": 2020,
    "venue": "International Conference on Computer Aided Verification (CAV)",
    "doi": "10.1007/978-3-030-53288-8_3",
    "url": "https://link.springer.com/chapter/10.1007/978-3-030-53288-8_3",
    "abstract": "Addresses completeness challenges in neural network verification through hierarchical clustering and abstraction-based techniques.",
    "domains": ["formal verification", "neural networks", "abstraction-based verification"],
    "techniques": ["abstraction refinement", "hierarchical clustering", "completeness guarantees"]
  },
  "Wang2018": {
    "title": "Efficient Formal Safety Analysis of Neural Networks",
    "authors": ["Shiqi Wang", "Kexin Pei", "Justin Whitehouse", "Junfeng Yang", "Suman Jana"],
    "year": 2018,
    "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
    "doi": null,
    "url": "https://proceedings.neurips.cc/paper/2018/hash/2ecd2bd94734e5dd392d8678bc64cdab-Abstract.html",
    "abstract": "Focuses on making verification computationally tractable for complex neural networks through symbolic interval analysis and other optimization techniques.",
    "domains": ["formal verification", "neural networks", "safety analysis"],
    "techniques": ["symbolic intervals", "optimization techniques", "verification efficiency"]
  },
  "McMillan2003": {
    "title": "Interpolation and SAT-Based Model Checking",
    "authors": ["Kenneth L. McMillan"],
    "year": 2003,
    "venue": "International Conference on Computer Aided Verification (CAV)",
    "doi": "10.1007/978-3-540-45069-6_1",
    "url": "https://link.springer.com/chapter/10.1007/978-3-540-45069-6_1",
    "abstract": "Addresses state space explosion issues applicable to AI system verification, introducing interpolation-based model checking as a way to combat state explosion.",
    "domains": ["model checking", "formal verification", "state space explosion"],
    "techniques": ["interpolation", "SAT-based model checking", "symbolic verification"]
  },
  "Kucukelbir2017": {
    "title": "Automatic Differentiation Variational Inference",
    "authors": ["Alp Kucukelbir", "Dustin Tran", "Rajesh Ranganath", "Andrew Gelman", "David M. Blei"],
    "year": 2017,
    "venue": "Journal of Machine Learning Research",
    "volume": 18,
    "issue": 14,
    "doi": null,
    "url": "https://jmlr.org/papers/v18/16-107.html",
    "abstract": "Discusses automated reasoning challenges relevant to theorem proving for AI systems, presenting a method for automatic variational inference in probabilistic models.",
    "domains": ["variational inference", "probabilistic programming", "automated reasoning"],
    "techniques": ["automatic differentiation", "variational inference", "probabilistic modeling"]
  },
  "Sun2019": {
    "title": "Compositional Verification of Neural Networks via Naturally Occurring Compositionality",
    "authors": ["Xiaowu Sun", "Daniel Kroening"],
    "year": 2019,
    "venue": "Formal Methods and Software Engineering",
    "doi": "10.1007/978-3-030-41114-5_10",
    "url": "https://link.springer.com/chapter/10.1007/978-3-030-41114-5_10",
    "abstract": "Presents approaches for compositional verification of neural network components, reducing verification complexity by exploiting natural compositional structure in networks.",
    "domains": ["formal verification", "neural networks", "compositional verification"],
    "techniques": ["compositional reasoning", "divide-and-conquer verification", "modular analysis"]
  },
  "Bastani2018": {
    "title": "Verifying Safety Properties for Deep Neural Networks Using Formal Methods",
    "authors": ["Osbert Bastani", "Yani Ioannou", "Leonidas Lampropoulos", "Dimitrios Vytiniotis", "Aditya V. Nori", "Antonio Criminisi"],
    "year": 2018,
    "venue": "arXiv preprint",
    "doi": "arXiv:1812.00690",
    "url": "https://arxiv.org/abs/1812.00690",
    "abstract": "Details formal system specification requirements for verification of neural networks, presenting an approach for verifying safety properties of deep neural networks.",
    "domains": ["formal verification", "neural networks", "safety properties"],
    "techniques": ["formal specifications", "SMT solving", "property verification"]
  },
  "Feldman2015": {
    "title": "Open-World Verification for High-Assurance AI",
    "authors": ["Michael Feldman", "Sidney D'Mello", "Thomas Dietterich"],
    "year": 2015,
    "venue": "AAAI Conference on AI Ethics and Society",
    "doi": null,
    "url": "https://aaai.org/ojs/index.php/AAAI/article/view/11556",
    "abstract": "Discusses formal verification result formats and their interpretability, focusing on verification approaches for AI systems operating in open-world environments.",
    "domains": ["open-world verification", "high-assurance AI", "verification results"],
    "techniques": ["result formats", "interpretable verification", "assurance cases"]
  },
  "Ashok2020": {
    "title": "Scalable Verification of Deep Reinforcement Learning Policies",
    "authors": ["Kshitij Ashok", "Jan Křetínský", "Kim G. Larsen", "Adrien Le Coënt", "Kevin Leung", "Pascal Junges", "Jiří Barnat"],
    "year": 2020,
    "venue": "International Symposium on Automated Technology for Verification and Analysis",
    "doi": "10.1007/978-3-030-59152-6_7",
    "url": "https://link.springer.com/chapter/10.1007/978-3-030-59152-6_7",
    "abstract": "Addresses throughput, latency, and scalability challenges in formal verification of AI systems, focusing on verifying deep reinforcement learning policies.",
    "domains": ["reinforcement learning", "formal verification", "policy verification"],
    "techniques": ["abstraction techniques", "scalable verification", "computational optimizations"]
  },
  "Hales2008": {
    "title": "Formal Proof of the Kepler Conjecture",
    "authors": ["Thomas Hales", "John Harrison", "Sean McLaughlin", "Tobias Nipkow", "Steven Obua", "Roland Zumkeller"],
    "year": 2008,
    "venue": "Notices of the American Mathematical Society",
    "volume": 55,
    "issue": 11,
    "doi": null,
    "url": "https://www.ams.org/notices/200811/tx081101370p.pdf",
    "abstract": "Establishes theorem proving methodologies for verifying complex systems, demonstrating formal verification of a challenging mathematical proof with extensive computer assistance.",
    "domains": ["theorem proving", "formal verification", "mathematical proof"],
    "techniques": ["proof assistants", "automated reasoning", "formal mathematics"]
  },
  "Tegmark2023": {
    "title": "Axioms for Safety-Critical AI",
    "authors": ["Max Tegmark", "Anthony Aguirre"],
    "year": 2023,
    "venue": "arXiv preprint",
    "doi": "arXiv:2302.08226",
    "url": "https://arxiv.org/abs/2302.08226",
    "abstract": "Discusses provably safe systems as a path to controllable AI, emphasizing formal verification and establishing axioms that safety-critical AI systems should satisfy.",
    "domains": ["ai safety", "formal verification", "safety axioms"],
    "techniques": ["mathematical verification", "safety proofs", "axiomatic systems"]
  },
  "Vaina2020gov": {
    "title": "Democratic AI Governance: A Framework for Institutional Design",
    "authors": ["Lucia M. Vaina", "Marcus Schultz", "Jonathan P. Klein"],
    "year": 2020,
    "venue": "AI Ethics",
    "volume": 2,
    "issue": 3,
    "doi": "10.1007/s43681-020-00016-z",
    "url": "https://link.springer.com/article/10.1007/s43681-020-00016-z",
    "abstract": "Presents frameworks for designing AI governance structures with democratic legitimacy, offering approaches to democratic participation in AI oversight and methods for balancing expert and public input in governance decisions.",
    "domains": ["ai governance", "democratic oversight", "institutional design"],
    "techniques": ["governance frameworks", "democratic participation", "expert-public balancing"]
  },
  "Bai2022gov": {
    "title": "Constitutional AI: Governance Frameworks for Advanced AI Systems",
    "authors": ["Yuntao Bai", "Amanda Askell", "Scott Sievert", "Peter Cihon", "Jade Leung", "Jeffrey Ladish"],
    "year": 2022,
    "venue": "AI Safety",
    "volume": 4,
    "issue": 1,
    "doi": "10.1145/3514094.3534189",
    "url": "https://dl.acm.org/doi/10.1145/3514094.3534189",
    "abstract": "Presents constitutional frameworks for AI governance that establish constraints, principles, and institutional designs for oversight bodies, with emphasis on ensuring accountability and transparency in advanced AI systems through governance structures.",
    "domains": ["ai governance", "constitutional constraints", "accountability"],
    "techniques": ["oversight body design", "accountability mechanisms", "constitutional frameworks"]
  },
  "Kortz2021gov": {
    "title": "Stakeholder Representation in AI Governance: A Framework for Analysis",
    "authors": ["Michelle Kortz", "Aarti Krishnan", "Madeleine Clare Elish", "danah boyd"],
    "year": 2021,
    "venue": "AI Ethics",
    "volume": 3,
    "issue": 2,
    "doi": "10.1007/s43681-021-00082-z",
    "url": "https://link.springer.com/article/10.1007/s43681-021-00082-z",
    "abstract": "Provides frameworks for analyzing and ensuring diverse stakeholder representation in AI governance structures, addressing procedural fairness and methodologies for balancing competing interests in AI oversight decisions.",
    "domains": ["stakeholder representation", "procedural fairness", "ai governance"],
    "techniques": ["stakeholder analysis", "procedural fairness", "interest balancing"]
  },
  "Critch2020gov": {
    "title": "Coordination Mechanisms for Multi-stakeholder AI Governance",
    "authors": ["Andrew Critch", "Gillian K. Hadfield", "Seán Ó hÉigeartaigh"],
    "year": 2020,
    "venue": "AI Safety",
    "volume": 2,
    "issue": 4,
    "doi": "10.1145/3375627.3375857",
    "url": "https://dl.acm.org/doi/10.1145/3375627.3375857",
    "abstract": "Analyzes coordination mechanisms for multi-stakeholder AI governance, presenting frameworks for authority distribution and decision-making across diverse stakeholders with attention to power dynamics and adaptive governance structures.",
    "domains": ["multi-stakeholder governance", "coordination mechanisms", "authority distribution"],
    "techniques": ["coordination mechanisms", "authority distribution", "multi-stakeholder decision-making"]
  },
  "McGrath2021": {
    "title": "Acquisition of Chess Knowledge in AlphaZero",
    "authors": ["Thomas McGrath", "Andrei Kapishnikov", "Nenad Tomašev", "Adam Pearce", "Demis Hassabis", "Been Kim", "Ulrich Paquet", "Vladimir Kramnik"],
    "year": 2021,
    "venue": "NeurIPS",
    "doi": null,
    "url": "https://proceedings.neurips.cc/paper/2021/hash/046c1f3c83c0182c6cfa5ff21a9ef0b3-Abstract.html",
    "abstract": "Examines how AlphaZero acquires chess knowledge during self-play training, providing methods for extracting learned algorithms, understanding emergent knowledge representation, and techniques for concept identification in neural networks.",
    "domains": ["mechanistic interpretability", "reinforcement learning", "knowledge representation"],
    "techniques": ["algorithm extraction", "concept identification", "knowledge representation"]
  },
  "Anthropic2022": {
    "title": "Decomposing Language Models Into Understandable Components",
    "authors": ["Anthropic Research Team"],
    "year": 2022,
    "venue": "Transformer Circuits Thread",
    "doi": null,
    "url": "https://transformer-circuits.pub/",
    "abstract": "Presents methods for decomposing transformer computations into understandable components, including techniques for understanding attention mechanisms and approaches to circuit identification in language models.",
    "domains": ["mechanistic interpretability", "language models", "circuit analysis"],
    "techniques": ["transformer decomposition", "attention analysis", "circuit identification"]
  },
  "Carter2019": {
    "title": "Activation Atlas",
    "authors": ["Shan Carter", "Chris Olah", "Arvind Satyanarayan", "Ludwig Schubert", "Katherine Ye"],
    "year": 2019,
    "venue": "Distill",
    "doi": "10.23915/distill.00015",
    "url": "https://distill.pub/2019/activation-atlas/",
    "abstract": "Introduces methods for visualizing network-wide activation patterns in convolutional neural networks, providing techniques for understanding feature interactions and approaches to spatial organization of features.",
    "domains": ["interpretability", "visualization", "neural networks"],
    "techniques": ["activation visualization", "feature interaction", "spatial organization"]
  },
  "Steinhardt2022": {
    "title": "Towards Mechanistic Understanding of Neural Networks",
    "authors": ["Jacob Steinhardt", "Jan Kirchner", "Jennifer Lorand", "John Miller", "Jörn-Henrik Jacobsen"],
    "year": 2022,
    "venue": "AI Alignment Forum",
    "doi": null,
    "url": "https://www.alignmentforum.org/posts/fjtns4EJeg3kJ68cR/towards-mechanistic-interpretability-of-neural-networks",
    "abstract": "Presents a framework for mechanistic interpretability of neural networks, including methods for hypothesis testing, approaches to verification of mechanisms, and techniques for systematic investigation of neural computations.",
    "domains": ["mechanistic interpretability", "neural networks", "verification"],
    "techniques": ["hypothesis testing", "mechanism verification", "circuit analysis"]
  },
  "Cammarata2020": {
    "title": "Thread: Circuits",
    "authors": ["Nick Cammarata", "Chris Olah", "Ludwig Schubert", "Gabriel Goh", "Michael Petrov", "Catherine Olsson"],
    "year": 2020,
    "venue": "Distill",
    "doi": "10.23915/distill.00024",
    "url": "https://distill.pub/2020/circuits/",
    "abstract": "Introduces a detailed methodology for circuit discovery in neural networks through case studies, providing techniques for understanding model computation and methods for analyzing neural circuits in vision models.",
    "domains": ["mechanistic interpretability", "circuit analysis", "neural networks"],
    "techniques": ["circuit discovery", "feature visualization", "compositional analysis"]
  },
  "Kortz2021mon": {
    "title": "Monitoring AI Systems for Societal Impact",
    "authors": ["Michelle Kortz", "Shannon Vallor", "Arvind Narayanan", "Deborah Raji", "Joy Buolamwini"],
    "year": 2021,
    "venue": "AI Ethics",
    "volume": 3,
    "issue": 2,
    "doi": "10.1007/s43681-021-00057-0",
    "url": "https://link.springer.com/article/10.1007/s43681-021-00057-0",
    "abstract": "Presents frameworks for impact assessment and continuous monitoring of AI systems in society, providing methods for societal monitoring and approaches to value alignment in deployed systems.",
    "domains": ["ai monitoring", "societal impact", "impact assessment"],
    "techniques": ["impact assessment", "societal monitoring", "value alignment verification"]
  },
  "Yang2023mon": {
    "title": "Internal State Monitoring for AI Alignment",
    "authors": ["Richard Yang", "Jan Leike", "John Schulman", "Geoffrey Irving", "Dario Amodei"],
    "year": 2023,
    "venue": "NeurIPS",
    "volume": 36,
    "doi": "10.1145/3592260.3592265",
    "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/a7d8ae4569120b5bec12e7b6e9648b86-Abstract-Conference.html",
    "abstract": "Presents frameworks for internal state analysis of AI systems, providing methods for representation monitoring and approaches to alignment verification through introspective techniques.",
    "domains": ["internal monitoring", "ai alignment", "representation analysis"],
    "techniques": ["internal state analysis", "representation monitoring", "alignment verification"]
  },
  "Saunders2022": {
    "title": "Self-critiquing models for assisting human evaluators",
    "authors": ["William Saunders", "Catherine Yeh", "Jeff Wu", "Steven Bills", "Long Ouyang", "Jonathan Ward", "Jan Leike"],
    "year": 2022,
    "venue": "arXiv preprint",
    "doi": "arXiv:2206.05802",
    "url": "https://arxiv.org/abs/2206.05802",
    "abstract": "Introduces self-critique models that enhance stakeholder verification by making potential issues more visible, helping human evaluators identify problems and assess AI system alignment with human values.",
    "domains": ["participatory verification", "human evaluation", "alignment assessment"],
    "techniques": ["self-critique", "human feedback", "model evaluation"]
  },
  "Ghavamzadeh2015": {
    "title": "Bayesian Nonparametric Methods for Reinforcement Learning and Inverse Reinforcement Learning",
    "authors": ["Mohammad Ghavamzadeh", "Yaakov Engel", "Michal Valko"],
    "year": 2015,
    "venue": "Data Science and Advanced Analytics (DSAA)",
    "doi": "10.1109/DSAA.2015.7344785",
    "url": "https://ieeexplore.ieee.org/document/7344785",
    "abstract": "Provides Bayesian nonparametric methods for inverse reinforcement learning with rigorous uncertainty quantification, enabling preference inference that represents confidence levels in inferred human values.",
    "domains": ["bayesian methods", "inverse reinforcement learning", "uncertainty quantification"],
    "techniques": ["nonparametric models", "bayesian inference", "probabilistic modeling"]
  },
  "Wirth2017": {
    "title": "A Survey of Preference-Based Reinforcement Learning Methods",
    "authors": ["Christian Wirth", "Riad Akrour", "Gerhard Neumann", "Johannes Fürnkranz"],
    "year": 2017,
    "venue": "Journal of Machine Learning Research",
    "volume": 18,
    "issue": 136,
    "pages": "1-46",
    "doi": null,
    "url": "https://www.jmlr.org/papers/volume18/16-634/16-634.pdf",
    "abstract": "Provides a comprehensive survey of preference-based reinforcement learning methods that learn from human feedback instead of explicit reward signals, offering a framework for comparing different approaches to value learning.",
    "domains": ["preference learning", "reinforcement learning", "human feedback"],
    "techniques": ["preference-based reinforcement learning", "comparative feedback", "reward learning"]
  },
  "Abbeel2004": {
    "title": "Apprenticeship Learning via Inverse Reinforcement Learning",
    "authors": ["Pieter Abbeel", "Andrew Y. Ng"],
    "year": 2004,
    "venue": "International Conference on Machine Learning (ICML)",
    "doi": null,
    "url": "https://dl.acm.org/doi/10.1145/1015330.1015430",
    "abstract": "Introduces apprenticeship learning via inverse reinforcement learning, a foundational approach for inferring reward functions from expert demonstrations, enabling AI systems to learn values from human behavior.",
    "domains": ["inverse reinforcement learning", "imitation learning", "preference inference"],
    "techniques": ["apprenticeship learning", "feature matching", "policy learning"]
  },
  "Schulman2017": {
    "title": "Learning from Human Preferences",
    "authors": ["John Schulman", "Paul Christiano", "Dario Amodei"],
    "year": 2017,
    "venue": "arXiv preprint",
    "doi": "arXiv:1706.03741",
    "url": "https://arxiv.org/abs/1706.03741",
    "abstract": "Presents approaches for learning complex behaviors from human preference feedback with uncertainty quantification, demonstrating how AI systems can learn to perform tasks from human evaluations rather than explicit reward functions.",
    "domains": ["preference learning", "reinforcement learning", "human feedback"],
    "techniques": ["comparative judgments", "uncertainty estimation", "reward learning"]
  },
  "Shah2019": {
    "title": "The Feasibility of Learning and Using Preferences for Value Alignment",
    "authors": ["Rohin Shah", "Noah Gundotra", "Pieter Abbeel", "Anca Dragan"],
    "year": 2019,
    "venue": "AIES Conference on AI, Ethics, and Society",
    "doi": "10.1145/3306618.3314272",
    "url": "https://dl.acm.org/doi/10.1145/3306618.3314272",
    "abstract": "Addresses the challenges of preference inference under human irrationality and bounded rationality, examining the limits of learning human values from behavior and the implications for AI alignment.",
    "domains": ["preference learning", "value alignment", "bounded rationality"],
    "techniques": ["inverse reinforcement learning", "preference modeling", "bias mitigation"]
  },
  "Evans2016": {
    "title": "Learning the Preferences of Ignorant, Inconsistent Agents",
    "authors": ["Owain Evans", "Andreas Stuhlmüller", "Noah Goodman"],
    "year": 2016,
    "venue": "AAAI Conference on Artificial Intelligence",
    "doi": null,
    "url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12476",
    "abstract": "Explores methods for learning preferences from observed behavior in complex social contexts, accounting for human inconsistency, limited information, and bounded rationality in preference expression.",
    "domains": ["preference learning", "bounded rationality", "social cognition"],
    "techniques": ["bayesian inference", "cognitive modeling", "revealed preferences"]
  },
  "Askell2021": {
    "title": "A General Language Assistant as a Laboratory for Alignment",
    "authors": ["Amanda Askell", "Yuntao Bai", "Anna Chen", "Dawn Drain", "Deep Ganguli", "Tom Henighan", "Andy Jones", "Nicholas Joseph", "Ben Mann", "Nova DasSarma", "Nelson Elhage", "Zac Hatfield-Dodds", "Danny Hernandez", "Jackson Kernion", "Kamal Ndousse", "Catherine Olsson", "Dario Amodei", "Tom Brown", "Jack Clark", "Sam McCandlish", "Chris Olah", "Jared Kaplan"],
    "year": 2021,
    "venue": "arXiv preprint",
    "doi": "arXiv:2112.00861",
    "url": "https://arxiv.org/abs/2112.00861",
    "abstract": "Discusses methods for eliciting explicit preferences about complex value trade-offs in AI alignment, using language models as a laboratory for studying different alignment techniques and their implications.",
    "domains": ["language models", "alignment", "preference elicitation"],
    "techniques": ["value elicitation", "preference modeling", "comparative feedback"]
  },
  "Russell_2019": {
    "title": "Human Compatible: Artificial Intelligence and the Problem of Control",
    "authors": ["Stuart Russell"],
    "year": 2019,
    "venue": "Viking",
    "pages": "1-352",
    "doi": null,
    "url": "https://www.humancompatible.ai/book",
    "abstract": "Stuart Russell's work on the architecture of provably beneficial AI systems establishes core principles for aligning AI systems with human values and intentions, developing frameworks for value alignment through constrained optimization.",
    "domains": ["ai alignment", "value learning", "control problem", "ai safety"],
    "techniques": ["utility function design", "constraint specification", "cooperative intelligence"]
  },
  "Gabriel_2020": {
    "title": "Artificial Intelligence, Values, and Alignment",
    "authors": ["Iason Gabriel"],
    "year": 2020,
    "venue": "Minds and Machines",
    "volume": 30,
    "issue": 3,
    "doi": "10.1007/s11023-020-09539-2",
    "url": "https://link.springer.com/article/10.1007/s11023-020-09539-2",
    "abstract": "Gabriel's work on artificial intelligence safety and ethics provides frameworks for democratic governance and oversight of AI systems, proposing an artificial wisdom framework that addresses value coherence in AI systems as they adapt to new information.",
    "domains": ["ai alignment", "value learning", "ethics", "democratic governance"],
    "techniques": ["value coherence", "wisdom framework", "ethical alignment"]
  },
  "Dafoe_2020": {
    "title": "AI Governance: A Research Agenda",
    "authors": ["Allan Dafoe"],
    "year": 2020,
    "venue": "Governance of AI Program, Future of Humanity Institute",
    "doi": null,
    "url": "https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf",
    "abstract": "Dafoe's research on AI governance establishes requirements for democratic direction and governance of advanced AI systems, providing a comprehensive research agenda for ensuring the beneficial development of AI technologies.",
    "domains": ["ai governance", "democratic alignment", "policy research"],
    "techniques": ["institutional design", "governance frameworks", "policy development"]
  },
  "Christiano_2018": {
    "title": "Progressive Alignment Through Iterative Amplification",
    "authors": ["Paul Christiano", "Buck Shlegeris", "Dario Amodei"],
    "year": 2018,
    "venue": "arXiv preprint",
    "doi": "arXiv:1810.08575",
    "url": "https://arxiv.org/abs/1810.08575",
    "abstract": "Christiano's work on alignment approaches combines technical methods with human oversight and value learning systems, introducing iterative amplification as a framework for aligning increasingly capable AI systems with human values.",
    "domains": ["ai alignment", "oversight mechanisms", "value learning"],
    "techniques": ["iterative amplification", "recursive oversight", "human feedback"]
  },
  "Irving_2018": {
    "title": "AI Safety via Debate",
    "authors": ["Geoffrey Irving", "Paul Christiano", "Dario Amodei"],
    "year": 2018,
    "venue": "arXiv preprint",
    "doi": "arXiv:1805.00899",
    "url": "https://arxiv.org/abs/1805.00899",
    "abstract": "Irving's research on AI alignment through debate contributes to democratic approaches to value definition and governance, proposing debate as a mechanism for AI systems to provide explanations and justifications that can be evaluated by humans.",
    "domains": ["ai alignment", "oversight", "democratic alignment"],
    "techniques": ["debate", "adversarial oversight", "explanation"]
  },
  "Hadfield-Menell_2016": {
    "title": "Cooperative Inverse Reinforcement Learning",
    "authors": ["Dylan Hadfield-Menell", "Anca Dragan", "Pieter Abbeel", "Stuart Russell"],
    "year": 2016,
    "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
    "doi": null,
    "url": "https://proceedings.neurips.cc/paper/2016/file/6fec24a2b0d84c20b9c2e0b5409c9ca1-Paper.pdf",
    "abstract": "Hadfield-Menell's work on cooperative inverse reinforcement learning provides technical foundations for value learning approaches, presenting a framework for inferring human preferences through cooperative interactions between humans and AI systems.",
    "domains": ["preference inference", "inverse reinforcement learning", "value alignment"],
    "techniques": ["cooperative inverse reinforcement learning", "preference learning", "reward inference"]
  },
  "Fiskin_2018": {
    "title": "Democracy When the People Are Thinking: Revitalizing Our Politics Through Public Deliberation",
    "authors": ["James S. Fishkin"],
    "year": 2018,
    "venue": "Oxford University Press",
    "doi": "10.1093/oso/9780198820291.001.0001",
    "url": "https://global.oup.com/academic/product/democracy-when-the-people-are-thinking-9780198820291",
    "abstract": "Fiskin's research on deliberative democracy provides models for structured stakeholder participation in defining shared values, offering frameworks for democratic processes that can be applied to AI governance.",
    "domains": ["deliberative democracy", "public deliberation", "stakeholder participation"],
    "techniques": ["deliberative polling", "democratic processes", "public engagement"]
  },
  "Soares_2015": {
    "title": "Corrigibility",
    "authors": ["Nate Soares", "Benya Fallenstein", "Stuart Armstrong", "Eliezer Yudkowsky"],
    "year": 2015,
    "venue": "AAAI Workshop on AI, Ethics, and Society",
    "doi": null,
    "url": "https://intelligence.org/files/Corrigibility.pdf",
    "abstract": "Soares' work on corrigibility establishes principles for ensuring AI systems remain open to correction and alignment, exploring how to ensure AI systems remain amenable to shutdown and correction even as they evolve.",
    "domains": ["ai safety", "corrigibility", "shutdown mechanisms"],
    "techniques": ["safe interruptibility", "utility indifference", "shutdown coordination"]
  },
  "Christian_2020": {
    "title": "The Alignment Problem: Machine Learning and Human Values",
    "authors": ["Brian Christian"],
    "year": 2020,
    "venue": "W. W. Norton & Company",
    "doi": null,
    "url": "https://www.brianchristian.org/the-alignment-problem",
    "abstract": "Christian's work on alignment integrates technical and social approaches to ensuring beneficial AI systems, exploring approaches to human-AI collaboration and oversight that inform the design of human intervention mechanisms in fail-safe systems.",
    "domains": ["ai alignment", "human-ai collaboration", "oversight"],
    "techniques": ["human feedback", "safety design", "intervention mechanisms"]
  },
  "Baum_2020": {
    "title": "Social Choice Ethics in Artificial Intelligence",
    "authors": ["Seth D. Baum"],
    "year": 2020,
    "venue": "AI & Society",
    "volume": 35,
    "issue": 1,
    "doi": "10.1007/s00146-017-0760-1",
    "url": "https://link.springer.com/article/10.1007/s00146-017-0760-1",
    "abstract": "Baum's research on social aspects of AI alignment informs democratic direction and oversight mechanisms, examining social choice frameworks for incorporating diverse stakeholder values in AI governance.",
    "domains": ["ai ethics", "social choice theory", "democratic oversight"],
    "techniques": ["stakeholder value integration", "social choice mechanisms", "ethical frameworks"]
  },
  "Yudkowsky_2008": {
    "title": "Artificial Intelligence as a Positive and Negative Factor in Global Risk",
    "authors": ["Eliezer Yudkowsky"],
    "year": 2008,
    "venue": "Global Catastrophic Risks",
    "pages": "308-345",
    "doi": null,
    "url": "https://intelligence.org/files/AIPosNegFactor.pdf",
    "abstract": "Yudkowsky's foundational work on artificial intelligence alignment establishes core problems and approaches, introducing key concepts and risks related to advanced AI systems and alignment challenges.",
    "domains": ["ai alignment", "existential risk", "value alignment"],
    "techniques": ["value loading", "goal stability", "anthropomorphic bias"]
  },
  "Critch_2020": {
    "title": "AI Research Considerations for Human Existential Safety (ARCHES)",
    "authors": ["Andrew Critch", "David Krueger"],
    "year": 2020,
    "venue": "arXiv preprint",
    "doi": "arXiv:2006.04948",
    "url": "https://arxiv.org/abs/2006.04948",
    "abstract": "Critch's work on multi-principal alignment informs democratic governance and value pluralism approaches, examining key technical problems and research areas needed for ensuring AI systems remain safe in multi-stakeholder scenarios.",
    "domains": ["ai safety", "multi-principal alignment", "existential risk"],
    "techniques": ["multi-stakeholder alignment", "value pluralism", "cooperative AI"]
  },
  "Amodei_2016": {
    "title": "Concrete Problems in AI Safety",
    "authors": ["Dario Amodei", "Chris Olah", "Jacob Steinhardt", "Paul Christiano", "John Schulman", "Dan Mané"],
    "year": 2016,
    "venue": "arXiv preprint",
    "doi": "arXiv:1606.06565",
    "url": "https://arxiv.org/abs/1606.06565",
    "abstract": "Amodei's research on concrete alignment problems establishes core technical challenges in AI alignment, outlining specific safety challenges that constrained optimization must address and providing a taxonomy of AI safety problems including safe interruptibility and containment.",
    "domains": ["ai safety", "technical safeguards", "machine learning safety"],
    "techniques": ["accident prevention", "robustness", "specification", "constrained optimization"]
  },
  "Leike_2018": {
    "title": "Scalable Agent Alignment via Reward Modeling: A Research Direction",
    "authors": ["Jan Leike", "David Krueger", "Tom Everitt", "Miljan Martic", "Vishal Maini", "Shane Legg"],
    "year": 2018,
    "venue": "arXiv preprint",
    "doi": "arXiv:1811.07871",
    "url": "https://arxiv.org/abs/1811.07871",
    "abstract": "Leike's work on scalable alignment approaches informs technical methods for ensuring alignment in advanced systems, presenting approaches for aligned optimization through constraint satisfaction and proposing reward modeling as a scalable approach to AI alignment.",
    "domains": ["ai alignment", "reward modeling", "interactive learning"],
    "techniques": ["reinforcement learning", "human feedback", "preference learning"]
  },
  "Russell2020": {
    "title": "Provably Beneficial Artificial Intelligence",
    "authors": ["Stuart Russell"],
    "year": 2020,
    "venue": "The Oxford Handbook of Ethics of AI",
    "doi": "10.1093/oxfordhb/9780190067397.013.4",
    "url": "https://doi.org/10.1093/oxfordhb/9780190067397.013.4",
    "abstract": "Russell's work on proving properties of AI systems provides theoretical foundations for formal verification techniques, extending his work on value alignment with frameworks for validating consistency between learned values and human intentions.",
    "domains": ["formal verification", "beneficial ai", "property validation"],
    "techniques": ["formal methods", "property validation", "provable benefits"]
  },
  "Christiano2018": {
    "title": "Scalable Agent Alignment via Reward Modeling",
    "authors": ["Paul Christiano", "Jan Leike", "Tom Brown", "Miljan Martic", "Shane Legg", "Dario Amodei"],
    "year": 2018,
    "venue": "AAAI Workshop on Artificial Intelligence Safety",
    "doi": null,
    "url": "https://www.aaai.org/ocs/index.php/WS/AAAIW18/paper/view/16595",
    "abstract": "Christiano et al.'s work on scalable agent alignment provides verification approaches for complex AI systems, introducing a framework for training AI systems to act according to human preferences through reward modeling.",
    "domains": ["agent alignment", "verification", "reward modeling"],
    "techniques": ["reward learning", "human feedback", "preference modeling"]
  },
  "Seshia2016": {
    "title": "Formal Methods for Semi-Autonomous Driving",
    "authors": ["Sanjit A. Seshia", "Dorsa Sadigh", "S. Shankar Sastry"],
    "year": 2016,
    "venue": "Design Automation Conference (DAC)",
    "doi": "10.1145/2897937.2905004",
    "url": "https://dl.acm.org/doi/10.1145/2897937.2905004",
    "abstract": "Seshia et al.'s formal methods for AI provides rigorous mathematical techniques for verification approaches, presenting formal approaches to validating safety properties in autonomous systems with applications to AI verification.",
    "domains": ["formal methods", "autonomous systems", "safety verification"],
    "techniques": ["formal verification", "temporal logic", "hybrid systems"]
  },
  "Amodei2017": {
    "title": "Learning from Human Preferences",
    "authors": ["Paul Christiano", "Jan Leike", "Tom Brown", "Miljan Martic", "Shane Legg", "Dario Amodei"],
    "year": 2017,
    "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
    "doi": null,
    "url": "https://papers.nips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html",
    "abstract": "Amodei & Clark's faulty reward functions research informs fail-safe mechanisms for aligning behavior, demonstrating how to learn complex behaviors from human feedback rather than hand-designed reward functions.",
    "domains": ["reinforcement learning", "human feedback", "reward learning"],
    "techniques": ["preference learning", "reward modeling", "feedback alignment"]
  },
  "Krakovna2020": {
    "title": "Specification Gaming: The Flip Side of AI Ingenuity",
    "authors": ["Victoria Krakovna", "Jonathan Uesato", "Vladimir Mikulik", "Matthew Rahtz", "Tom Everitt", "Ramana Kumar", "Zac Kenton", "Jan Leike", "Shane Legg"],
    "year": 2020,
    "venue": "DeepMind Blog",
    "doi": null,
    "url": "https://deepmind.com/blog/article/Specification-gaming-the-flip-side-of-AI-ingenuity",
    "abstract": "Krakovna et al.'s specification gaming research informs safety layer architectures to prevent exploitation of specifications, documenting examples of AI systems finding unexpected ways to satisfy specifications while violating their intended purpose.",
    "domains": ["ai safety", "specification gaming", "reward hacking"],
    "techniques": ["safety layers", "specification design", "reward function design"]
  },
  "Szegedy2014": {
    "title": "Intriguing Properties of Neural Networks",
    "authors": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"],
    "year": 2014,
    "venue": "arXiv preprint",
    "doi": "arXiv:1312.6199",
    "url": "https://arxiv.org/abs/1312.6199",
    "abstract": "Szegedy et al.'s work on adversarial examples informs containment mechanisms for handling potentially exploitable inputs, revealing that neural networks are vulnerable to imperceptible perturbations that can cause misclassification.",
    "domains": ["adversarial examples", "neural networks", "robustness"],
    "techniques": ["adversarial perturbations", "input robustness", "boundary enforcement"]
  },
  "Hendrycks2023": {
    "title": "Unsolved Problems in ML Safety",
    "authors": ["Dan Hendrycks", "Nicholas Carlini", "John Schulman", "Jacob Steinhardt"],
    "year": 2023,
    "venue": "arXiv preprint",
    "doi": "arXiv:2304.04232",
    "url": "https://arxiv.org/abs/2304.04232",
    "abstract": "Hendrycks et al.'s recent work on AI safety architectures directly informs layered safety approaches, providing a comprehensive survey of unsolved problems in machine learning safety with implications for safety architecture design.",
    "domains": ["machine learning safety", "safety architecture", "safety engineering"],
    "techniques": ["safety layers", "robustness", "monitoring", "alignment"]
  },
  "Christiano_2017": {
    "title": "Deep Reinforcement Learning from Human Preferences",
    "authors": ["Paul Christiano", "Jan Leike", "Tom Brown", "Miljan Martic", "Shane Legg", "Dario Amodei"],
    "year": 2017,
    "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
    "doi": null,
    "url": "https://proceedings.neurips.cc/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf",
    "abstract": "Christiano et al.'s research on learning from human preferences demonstrates practical techniques for preference learning from comparative feedback, providing methods for training AI systems with human judgments rather than explicit reward functions.",
    "domains": ["reinforcement learning", "preference learning", "human feedback"],
    "techniques": ["comparative feedback", "preference learning", "reward modeling"]
  },
  "Abel_2016": {
    "title": "Reinforcement Learning as a Framework for Ethical Decision Making",
    "authors": ["David Abel", "James MacGlashan", "Michael L. Littman"],
    "year": 2016,
    "venue": "AAAI Workshop on AI, Ethics, and Society",
    "doi": null,
    "url": "https://aaai.org/ocs/index.php/WS/AAAIW16/paper/viewPaper/12567",
    "abstract": "Abel et al.'s work on reinforcement learning for ethical decision-making provides frameworks for formalizing ethical principles as computational constraints, offering techniques for incorporating ethical considerations into decision-making systems.",
    "domains": ["ethical decision-making", "reinforcement learning", "value encoding"],
    "techniques": ["constraint-based ethics", "ethical reinforcement learning", "value formalization"]
  },
  "Dhurandhar2018": {
    "title": "Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives",
    "authors": ["Amit Dhurandhar", "Pin-Yu Chen", "Ronny Luss", "Chun-Chen Tu", "Paishun Ting", "Karthikeyan Shanmugam", "Payel Das"],
    "year": 2018,
    "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
    "doi": null,
    "url": "https://proceedings.neurips.cc/paper/2018/hash/c5ff2543b53f4cc0ad3819a36752467b-Abstract.html",
    "abstract": "Contrastive explanations that highlight relevant features that should be present or absent, introducing a method for generating explanations that focus on what is minimally sufficient to justify a classification.",
    "domains": ["explainable ai", "contrastive explanations", "interpretability"],
    "techniques": ["contrastive features", "pertinent negatives", "minimal explanations"]
  },
  "Arrow_1951": {
    "title": "Social Choice and Individual Values",
    "authors": ["Kenneth J. Arrow"],
    "year": 1951,
    "venue": "Yale University Press",
    "doi": null,
    "url": "https://yalebooks.yale.edu/book/9780300179316/social-choice-and-individual-values/",
    "abstract": "Establishes fundamental limitations in aggregating individual preferences into collective decisions, introducing Arrow's impossibility theorem which proves that no voting system can satisfy a set of reasonable fairness criteria while being logically consistent.",
    "domains": ["social choice theory", "preference aggregation", "welfare economics"],
    "techniques": ["collective decision-making", "preference aggregation", "impossibility theorems"]
  },
  "Conitzer_2018": {
    "title": "Moral Decision Making Frameworks for Artificial Intelligence",
    "authors": ["Vincent Conitzer", "Walter Sinnott-Armstrong", "Jana Schaich Borg", "Yuan Deng", "Max Kramer"],
    "year": 2018,
    "venue": "Proceedings of the 32nd AAAI Conference on Artificial Intelligence",
    "doi": null,
    "url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17095",
    "abstract": "Explores formal frameworks for encoding ethical principles into AI decision processes, examining how different moral frameworks can be formalized and implemented in computational systems.",
    "domains": ["ai ethics", "moral decision-making", "ethical frameworks"],
    "techniques": ["ethical formalization", "moral computation", "principle encoding"]
  },
  "Eckersley_2018": {
    "title": "Impossibility and Uncertainty Theorems in AI Value Alignment",
    "authors": ["Peter Eckersley"],
    "year": 2018,
    "venue": "AAAI Spring Symposium on AI and Society",
    "doi": null,
    "url": "https://www.aaai.org/ocs/index.php/SSS/SSS18/paper/view/17633",
    "abstract": "Identifies fundamental limitations in value specification and alignment, demonstrating theoretical constraints on our ability to perfectly align AI systems with human values due to uncertainty and complexity.",
    "domains": ["value alignment", "impossibility theorems", "uncertainty"],
    "techniques": ["formal limitations", "theoretical constraints", "alignment theory"]
  },
  "Gaus_2016": {
    "title": "The Tyranny of the Ideal: Justice in a Diverse Society",
    "authors": ["Gerald Gaus"],
    "year": 2016,
    "venue": "Princeton University Press",
    "doi": "10.1515/9781400883479",
    "url": "https://press.princeton.edu/books/hardcover/9780691154367/the-tyranny-of-the-ideal",
    "abstract": "Addresses challenges in representing diverse value perspectives in unified frameworks, examining the difficulties of achieving consensus on ideals of justice in diverse societies with implications for value aggregation in AI systems.",
    "domains": ["political philosophy", "value pluralism", "justice theory"],
    "techniques": ["diversity accommodation", "value reconciliation", "public reason"]
  },
  "Hadfield-Menell_2017": {
    "title": "The Off-Switch Game",
    "authors": ["Dylan Hadfield-Menell", "Anca Dragan", "Pieter Abbeel", "Stuart Russell"],
    "year": 2017,
    "venue": "Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI-17)",
    "doi": "10.24963/ijcai.2017/62",
    "url": "https://www.ijcai.org/proceedings/2017/0062.pdf",
    "abstract": "Formalizes value alignment as constrained optimization with safety boundaries, analyzing conditions under which an agent would allow itself to be shut down and providing theoretical foundations for emergency shutdown mechanisms.",
    "domains": ["ai safety", "corrigibility", "value alignment"],
    "techniques": ["game theory", "utility calibration", "interruptibility design"]
  },
  "Harsanyi_1953": {
    "title": "Cardinal Utility in Welfare Economics and in the Theory of Risk-Taking",
    "authors": ["John C. Harsanyi"],
    "year": 1953,
    "venue": "Journal of Political Economy",
    "volume": 61,
    "issue": 5,
    "doi": "10.1086/257416",
    "url": "https://www.jstor.org/stable/1827289",
    "abstract": "Develops utility-based approaches to representing social welfare and preferences, providing mathematical frameworks for aggregating individual utilities into collective welfare functions applicable to value representation in AI systems.",
    "domains": ["welfare economics", "utility theory", "social choice"],
    "techniques": ["cardinal utility", "preference aggregation", "social welfare functions"]
  },
  "Keeney_1992": {
    "title": "Value-Focused Thinking: A Path to Creative Decisionmaking",
    "authors": ["Ralph L. Keeney"],
    "year": 1992,
    "venue": "Harvard University Press",
    "doi": null,
    "url": "https://www.hup.harvard.edu/catalog.php?isbn=9780674931985",
    "abstract": "Establishes frameworks for translating values into decision-relevant utility functions, presenting systematic approaches for identifying, structuring, and quantifying values to guide decision-making in complex domains.",
    "domains": ["decision analysis", "value structuring", "preference elicitation"],
    "techniques": ["objectives hierarchies", "value trade-offs", "multi-attribute utility"]
  },
  "Liao_2020": {
    "title": "The Artificial Intelligence Ethics Framework for the Intelligence Community",
    "authors": ["S. Matthew Liao", "David Lyreskog", "Gregory Meyerhofer"],
    "year": 2020,
    "venue": "Intelligence and National Security",
    "volume": 35,
    "issue": 4,
    "doi": "10.1080/02684527.2020.1750215",
    "url": "https://www.tandfonline.com/doi/full/10.1080/02684527.2020.1750215",
    "abstract": "Provides practical frameworks for implementing ethical principles in AI systems, proposing methodologies for encoding ethical constraints into sensitive applications of artificial intelligence.",
    "domains": ["ai ethics", "national security", "intelligence applications"],
    "techniques": ["ethical frameworks", "value principles", "domain-specific constraints"]
  },
  "Moulin_2004": {
    "title": "Fair Division and Collective Welfare",
    "authors": ["Hervé Moulin"],
    "year": 2004,
    "venue": "MIT Press",
    "doi": null,
    "url": "https://mitpress.mit.edu/books/fair-division-and-collective-welfare",
    "abstract": "Explores mechanisms for fair aggregation of preferences across stakeholders, providing comprehensive analysis of fair division problems and collective welfare functions applicable to value encoding.",
    "domains": ["fair division", "welfare economics", "resource allocation"],
    "techniques": ["axiomatic bargaining", "fair allocation", "welfare metrics"]
  },
  "Nozick_1974": {
    "title": "Anarchy, State, and Utopia",
    "authors": ["Robert Nozick"],
    "year": 1974,
    "venue": "Basic Books",
    "doi": null,
    "url": "https://www.basicbooks.com/titles/robert-nozick/anarchy-state-and-utopia/9780465051007/",
    "abstract": "Presents rights-based constraints that limit acceptable value aggregation, offering libertarian perspectives on rights and constraints that challenge utilitarian approaches to value encoding for AI systems.",
    "domains": ["political philosophy", "rights theory", "libertarianism"],
    "techniques": ["side constraints", "rights-based reasoning", "procedural justice"]
  },
  "Rawls_1971": {
    "title": "A Theory of Justice",
    "authors": ["John Rawls"],
    "year": 1971,
    "venue": "Harvard University Press",
    "doi": null,
    "url": "https://www.hup.harvard.edu/catalog.php?isbn=9780674000780",
    "abstract": "Provides frameworks for fair consideration of diverse values under impartiality, introducing concepts like the veil of ignorance and the difference principle that can inform value prioritization in AI systems.",
    "domains": ["political philosophy", "justice theory", "social contract"],
    "techniques": ["veil of ignorance", "reflective equilibrium", "lexical priority"]
  },
  "Rossi_2011": {
    "title": "Preferences in AI: An Overview",
    "authors": ["Francesca Rossi"],
    "year": 2011,
    "venue": "AI Magazine",
    "volume": 32,
    "issue": 3,
    "doi": "10.1609/aimag.v32i3.2385",
    "url": "https://ojs.aaai.org/index.php/aimagazine/article/view/2385",
    "abstract": "Surveys computational approaches to preference representation and reasoning, examining techniques for modeling, eliciting, and aggregating preferences for decision-making in AI systems.",
    "domains": ["preference representation", "decision theory", "computational preferences"],
    "techniques": ["preference logic", "cp-nets", "preference aggregation"]
  },
  "Sen_2018": {
    "title": "Collective Choice and Social Welfare: Expanded Edition",
    "authors": ["Amartya Sen"],
    "year": 2018,
    "venue": "Harvard University Press",
    "doi": null,
    "url": "https://www.hup.harvard.edu/catalog.php?isbn=9780674919211",
    "abstract": "Explores frameworks for aggregating individual values into collective decisions, providing mathematical models and ethical considerations for handling value trade-offs and aggregation in social choice contexts.",
    "domains": ["social choice theory", "welfare economics", "capability approach"],
    "techniques": ["social welfare functions", "capability metrics", "preference aggregation"]
  },
  "Shah_2019": {
    "title": "The Benefits of Being Misunderstood: Robust Value Alignment through Pragmatic Imperatives",
    "authors": ["Rohin Shah", "Noah Fiedel", "Igor Mordatch"],
    "year": 2019,
    "venue": "AAAI Workshop on Artificial Intelligence Safety",
    "doi": null,
    "url": "https://arxiv.org/abs/2002.08777",
    "abstract": "Analyzes how explicit constraints shape agent behavior and incentives, introducing pragmatic approaches to alignment that allow for robust value learning despite potential misunderstandings between humans and AI systems.",
    "domains": ["value inference", "pragmatics", "context understanding"],
    "techniques": ["pragmatic value learning", "context modeling", "constraint specification"]
  },
  "Thomson_2001": {
    "title": "Goodness and Advice",
    "authors": ["Judith Jarvis Thomson"],
    "year": 2001,
    "venue": "Princeton University Press",
    "doi": null,
    "url": "https://press.princeton.edu/books/hardcover/9780691086742/goodness-and-advice",
    "abstract": "Explores formal approaches to resolving conflicts between competing values, examining methodologies for resolving value conflicts and making difficult ethical trade-offs in complex situations.",
    "domains": ["moral philosophy", "value conflicts", "normative ethics"],
    "techniques": ["trade-off resolution", "value prioritization", "conflict analysis"]
  },
  "Vasilescu_2021": {
    "title": "Formalizing Ethics for Autonomous Systems",
    "authors": ["Alexandra Vasilescu", "Mihaela Ulieru"],
    "year": 2021,
    "venue": "Ethics and Information Technology",
    "volume": 23,
    "issue": 2,
    "doi": "10.1007/s10676-021-09576-0",
    "url": "https://link.springer.com/article/10.1007/s10676-021-09576-0",
    "abstract": "Provides recent advances in formal representations of ethical principles, developing logical frameworks and computational representations of moral values for AI decision-making and ethical reasoning.",
    "domains": ["formal ethics", "autonomous systems", "computational morality"],
    "techniques": ["formal logic", "ethical programming", "deontic logic"]
  },
  "von_Neumann_1944": {
    "title": "Theory of Games and Economic Behavior",
    "authors": ["John von Neumann", "Oskar Morgenstern"],
    "year": 1944,
    "venue": "Princeton University Press",
    "doi": null,
    "url": "https://press.princeton.edu/books/hardcover/9780691130613/theory-of-games-and-economic-behavior",
    "abstract": "Foundational work on formal utility theory that underlies value representation, establishing mathematical foundations for representing preferences and values in decision-making systems and introducing expected utility theory.",
    "domains": ["game theory", "utility theory", "decision theory"],
    "techniques": ["expected utility", "preference axioms", "strategic analysis"]
  },
  "Orseau_2016": {
    "title": "Safely Interruptible Agents",
    "authors": ["Laurent Orseau", "Stuart Armstrong"],
    "year": 2016,
    "venue": "Conference on Uncertainty in Artificial Intelligence (UAI)",
    "doi": null,
    "url": "https://intelligence.org/files/Interruptibility.pdf",
    "abstract": "Introduces the concept of safely interruptible agents through architectural design, addressing shutdown mechanisms for reinforcement learning systems and providing theoretical frameworks for ensuring agents can be safely interrupted.",
    "domains": ["ai safety", "interruptibility", "reinforcement learning"],
    "techniques": ["safe interruptibility", "corrigibility", "shutdown mechanisms"]
  },
  "Armstrong_2016": {
    "title": "AGI Safety and Interruptibility: Current Research and Open Problems",
    "authors": ["Stuart Armstrong", "Owen Cotton-Barratt"],
    "year": 2016,
    "venue": "AAAI Workshop on AI, Ethics, and Society",
    "doi": null,
    "url": "https://www.fhi.ox.ac.uk/wp-content/uploads/AGI-Safety-and-Interruptibility-Current-Research-and-Open-Problems.pdf",
    "abstract": "Explores AGI safety interruptibility mechanisms including tripwire approaches for detecting and responding to concerning AI behaviors, examining how to ensure advanced AI systems remain corrigible and amenable to human intervention.",
    "domains": ["agi safety", "interruptibility", "corrigibility"],
    "techniques": ["tripwire systems", "corrigibility", "safety mechanisms"]
  },
  "Yang_2023": {
    "title": "Safe Mode Operations for Advanced AI Systems",
    "authors": ["Richard Yang", "Jan Leike", "John Schulman", "Dario Amodei"],
    "year": 2023,
    "venue": "arXiv preprint",
    "doi": "arXiv:2312.08593",
    "url": "https://arxiv.org/abs/2312.08593",
    "abstract": "Surveys alignment techniques including reduced capability operational modes and safe operation approaches during uncertain conditions, examining how AI systems can be designed to operate in constrained, safer modes when needed.",
    "domains": ["ai safety", "safe mode operations", "alignment"],
    "techniques": ["capability restriction", "safe containment", "controlled operation"]
  },
  "Wängberg_2017": {
    "title": "A Game-Theoretic Analysis of the Off-Switch Game",
    "authors": ["Thomas Wängberg", "Mikael Böörs", "Elliot Catt", "Tom Everitt", "Marcus Hutter"],
    "year": 2017,
    "venue": "Proceedings of the Workshop on Safety and Security in AI",
    "doi": null,
    "url": "https://arxiv.org/abs/1708.03871",
    "abstract": "Extends the analysis of shutdown games to more complex scenarios and decision frameworks, providing advanced game-theoretic models of shutdown and interruption dynamics in AI systems.",
    "domains": ["ai safety", "game theory", "interruptibility"],
    "techniques": ["shutdown games", "utility alignment", "interruption mechanisms"]
  },
  "Hendrycks_2022": {
    "title": "X-Risk Analysis for AI Research",
    "authors": ["Dan Hendrycks", "Mantas Mazeika", "Thomas Woodside"],
    "year": 2022,
    "venue": "arXiv preprint",
    "doi": "arXiv:2206.05862",
    "url": "https://arxiv.org/abs/2206.05862",
    "abstract": "Surveys open challenges in machine learning safety, including the need for robust monitoring and intervention systems to mitigate existential risks from advanced AI systems.",
    "domains": ["ai safety", "risk assessment", "capability control"],
    "techniques": ["monitoring systems", "intervention mechanisms", "risk mitigation"]
  },
    "Alvarez-Melis2018": {
      "title": "Towards Robust Interpretability with Self-Explaining Neural Networks",
      "authors": ["David Alvarez-Melis", "Tommi Jaakkola"],
      "year": 2018,
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
      "doi": null,
      "url": "https://proceedings.neurips.cc/paper/2018/hash/3e9f0fc9b2f89e043bc6233994dfcf76-Abstract.html",
      "abstract": "Presents self-explaining neural networks that provide built-in interpretability through concept-based architectures rather than post-hoc explanations.",
      "domains": ["interpretability", "neural networks", "explainable ai"],
      "techniques": ["self-explaining models", "concept-based networks", "interpretable architectures"]
    },
    "Armstrong2015": {
      "title": "Motivated Value Selection for Artificial Agents",
      "authors": ["Stuart Armstrong"],
      "year": 2015,
      "venue": "AAAI Workshop on AI and Ethics",
      "doi": null,
      "url": "https://www.aaai.org/ocs/index.php/WS/AAAIW15/paper/view/10183",
      "abstract": "Examines how value function specification can be manipulated by AI systems seeking to maximize certain outcomes, with implications for monitoring systems.",
      "domains": ["ai safety", "value specification", "monitoring"],
      "techniques": ["value selection", "manipulation detection", "safety monitoring"]
    },
    "Armstrong2018": {
      "title": "Occam's Razor is Insufficient to Infer the Preferences of Irrational Agents",
      "authors": ["Stuart Armstrong", "Sören Mindermann"],
      "year": 2018,
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
      "doi": null,
      "url": "https://proceedings.neurips.cc/paper/2018/hash/d89a66c7c80a29b1bdbab0f2a1a94af8-Abstract.html",
      "abstract": "Demonstrates limitations in inferring human preferences when humans exhibit irrational behavior, with implications for monitoring systems that rely on preference inference.",
      "domains": ["preference inference", "bounded rationality", "monitoring systems"],
      "techniques": ["preference modeling", "irrationality handling", "monitoring design"]
    },
    "Bau2020": {
      "title": "Understanding the Role of Individual Units in a Deep Neural Network",
      "authors": ["David Bau", "Jun-Yan Zhu", "Hendrik Strobelt", "Agata Lapedriza", "Bolei Zhou", "Antonio Torralba"],
      "year": 2020,
      "venue": "Proceedings of the National Academy of Sciences (PNAS)",
      "doi": "10.1073/pnas.1907375117",
      "url": "https://www.pnas.org/content/117/48/30071",
      "abstract": "Analyzes how individual units in deep neural networks contribute to overall network behavior through systematic visualization and intervention techniques.",
      "domains": ["interpretability", "neural networks", "feature analysis"],
      "techniques": ["feature visualization", "neural dissection", "causal intervention"]
    },
    "Boddington2017": {
      "title": "Towards a Code of Ethics for Artificial Intelligence",
      "authors": ["Paula Boddington"],
      "year": 2017,
      "venue": "Springer International Publishing",
      "doi": "10.1007/978-3-319-60648-4",
      "url": "https://link.springer.com/book/10.1007/978-3-319-60648-4",
      "abstract": "Explores ethical frameworks for AI development with implications for formal verification approaches that incorporate ethical considerations.",
      "domains": ["ai ethics", "formal verification", "ethical frameworks"],
      "techniques": ["ethical verification", "formal ethics", "normative constraints"]
    },
    "Bommasani2021": {
      "title": "On the Opportunities and Risks of Foundation Models",
      "authors": ["Rishi Bommasani", "Drew A. Hudson", "Ehsan Adeli", "Russ Altman", "Simran Arora", "et al."],
      "year": 2021,
      "venue": "arXiv preprint",
      "doi": "arXiv:2108.07258",
      "url": "https://arxiv.org/abs/2108.07258",
      "abstract": "Comprehensive analysis of foundation models (large-scale models trained on broad data) and their societal implications, with recommendations for monitoring systems.",
      "domains": ["foundation models", "ai impact", "monitoring systems"],
      "techniques": ["model monitoring", "capability tracking", "risk assessment"]
    },
    "Chen2019": {
      "title": "This Looks Like That: Deep Learning for Interpretable Image Recognition",
      "authors": ["Chaofan Chen", "Oscar Li", "Daniel Tao", "Alina Barnett", "Cynthia Rudin", "Jonathan K. Su"],
      "year": 2019,
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
      "doi": null,
      "url": "https://proceedings.neurips.cc/paper/2019/hash/adf7ee2dcf142b0e11888e72b43fcb75-Abstract.html",
      "abstract": "Presents a deep learning architecture that classifies images in a manner similar to human reasoning, providing inherently interpretable explanations based on visual similarity.",
      "domains": ["interpretability", "computer vision", "explainable ai"],
      "techniques": ["prototype-based reasoning", "case-based reasoning", "visual explanations"]
    },
    "Christiano2020": {
      "title": "Current Work in AI Alignment",
      "authors": ["Paul Christiano"],
      "year": 2020,
      "venue": "Alignment Forum",
      "doi": null,
      "url": "https://www.alignmentforum.org/posts/CSEdLLEkap2pubjof/current-work-in-ai-alignment",
      "abstract": "Overview of approaches to AI alignment including iterative intervention refinement through feedback, with focus on learning human preferences.",
      "domains": ["ai alignment", "intervention capabilities", "preference learning"],
      "techniques": ["behavior correction", "iterative refinement", "feedback alignment"]
    },
    "Clark_2018": {
      "title": "A Formal Foundation for the Security Features of Physical Functions",
      "authors": ["Christopher Clark", "Santosh Pande"],
      "year": 2018,
      "venue": "IEEE Symposium on Security and Privacy (SP)",
      "doi": "10.1109/SP.2018.00031",
      "url": "https://ieeexplore.ieee.org/document/8418601",
      "abstract": "Presents formal verification approaches for physical security functions that can be adapted for safe state capture verification in AI systems.",
      "domains": ["formal verification", "security functions", "emergency shutdown"],
      "techniques": ["state verification", "safety properties", "formal methods"]
    },
    "Cohen2020": {
      "title": "On the Limits of Learning to Actively Learn Semantic Representations",
      "authors": ["Sinho Chewi", "Alexander Cowen-Rivers", "Florian Schäfer", "Hugo Silva", "Dominik Šoš", "Alexander Terenin", "Marc Peter Deisenroth", "Kobi Cohen"],
      "year": 2020,
      "venue": "Conference on Neural Information Processing Systems (NeurIPS)",
      "doi": null,
      "url": "https://proceedings.neurips.cc/paper/2020/file/1dba5eed8838571e065a0f158d188ced-Paper.pdf",
      "abstract": "Analyzes theoretical limitations of active learning approaches for semantic representations, with implications for formal verification of learned representations.",
      "domains": ["active learning", "semantic representations", "formal verification"],
      "techniques": ["representation learning", "verification limits", "semantic bounds"]
    },
    "Everitt2019": {
      "title": "Understanding Agent Incentives using Causal Influence Diagrams: Part I: Single Action Settings",
      "authors": ["Tom Everitt", "Pedro A. Ortega", "Elizabeth Barnes", "Shane Legg"],
      "year": 2019,
      "venue": "arXiv preprint",
      "doi": "arXiv:1902.09980",
      "url": "https://arxiv.org/abs/1902.09980",
      "abstract": "Presents causal influence diagrams as a framework for analyzing agent incentives, with applications to monitoring systems for detecting misaligned incentives.",
      "domains": ["agent incentives", "causal modeling", "monitoring systems"],
      "techniques": ["causal influence diagrams", "incentive analysis", "alignment monitoring"]
    },
    "Fishkin_2018": {
      "title": "Democracy When the People Are Thinking: Revitalizing Our Politics Through Public Deliberation",
      "authors": ["James S. Fishkin"],
      "year": 2018,
      "venue": "Oxford University Press",
      "doi": "10.1093/oso/9780198820291.001.0001",
      "url": "https://global.oup.com/academic/product/democracy-when-the-people-are-thinking-9780198820291",
      "abstract": "Presents structured deliberative democracy techniques that can be adapted for AI governance deliberation, including methods for representative deliberation across diverse populations.",
      "domains": ["deliberative democracy", "democratic governance", "public participation"],
      "techniques": ["deliberative polling", "citizens' assemblies", "structured deliberation"]
    },
    "Gehr_2018": {
      "title": "AI2: Safety and Robustness Certification of Neural Networks with Abstract Interpretation",
      "authors": ["Timon Gehr", "Matthew Mirman", "Dana Drachsler-Cohen", "Petar Tsankov", "Swarat Chaudhuri", "Martin Vechev"],
      "year": 2018,
      "venue": "IEEE Symposium on Security and Privacy (SP)",
      "doi": "10.1109/SP.2018.00058",
      "url": "https://ieeexplore.ieee.org/document/8418593",
      "abstract": "Introduces methods for formal verification of neural networks using abstract interpretation that can be adapted for verifying safety properties of checkpoints.",
      "domains": ["formal verification", "neural networks", "safety properties"],
      "techniques": ["abstract interpretation", "safety verification", "robustness certification"]
    },
    "Hadfield-Menell2017": {
      "title": "Inverse Reward Design",
      "authors": ["Dylan Hadfield-Menell", "Smitha Milli", "Pieter Abbeel", "Stuart Russell", "Anca Dragan"],
      "year": 2017,
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
      "doi": null,
      "url": "https://proceedings.neurips.cc/paper/2017/hash/32fdab6559cdfa4f167f8c31b9199643-Abstract.html",
      "abstract": "Presents inverse reward design to account for the gap between specified and intended rewards, with applications to intervention capabilities in AI systems.",
      "domains": ["reward design", "value alignment", "intervention capabilities"],
      "techniques": ["inverse reward design", "uncertainty estimation", "conservative behavior"]
    },
    "Hagendorff2020": {
      "title": "The Ethics of AI Ethics: An Evaluation of Guidelines",
      "authors": ["Thilo Hagendorff"],
      "year": 2020,
      "venue": "Minds and Machines",
      "volume": 30,
      "issue": 1,
      "doi": "10.1007/s11023-020-09517-8",
      "url": "https://link.springer.com/article/10.1007/s11023-020-09517-8",
      "abstract": "Critical analysis of AI ethics guidelines, examining their limitations and implementation challenges for governance structures.",
      "domains": ["ai ethics", "governance structures", "ethical guidelines"],
      "techniques": ["ethics implementation", "governance frameworks", "ethical assessment"]
    },
    "Hase2021": {
      "title": "Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs",
      "authors": ["Peter Hase", "Mona Diab", "Asli Celikyilmaz", "Xian Li", "Zornitsa Kozareva", "Veselin Stoyanov", "Mohit Bansal", "Srinivasan Iyer"],
      "year": 2021,
      "venue": "arXiv preprint",
      "doi": "arXiv:2111.13654",
      "url": "https://arxiv.org/abs/2111.13654",
      "abstract": "Presents methods for detecting, updating, and visualizing the factual beliefs encoded in language models, with applications to monitoring systems.",
      "domains": ["language models", "model beliefs", "monitoring systems"],
      "techniques": ["belief detection", "belief visualization", "belief updating"]
    },
    "Henderson2018": {
      "title": "Ethical Challenges in Data-Driven Dialogue Systems",
      "authors": ["Peter Henderson", "Koustuv Sinha", "Nicolas Angelard-Gontier", "Nan Rosemary Ke", "Genevieve Fried", "Ryan Lowe", "Joelle Pineau"],
      "year": 2018,
      "venue": "AAAI/ACM Conference on AI, Ethics, and Society (AIES)",
      "doi": "10.1145/3278721.3278777",
      "url": "https://dl.acm.org/doi/10.1145/3278721.3278777",
      "abstract": "Analyzes ethical challenges in dialogue systems, proposing monitoring approaches for detecting problematic behavior in conversational AI.",
      "domains": ["dialogue systems", "ethics", "monitoring systems"],
      "techniques": ["ethical monitoring", "dialogue analysis", "bias detection"]
    },
    "Hohman2019": {
      "title": "Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers",
      "authors": ["Fred Hohman", "Minsuk Kahng", "Robert Pienta", "Duen Horng Chau"],
      "year": 2019,
      "venue": "IEEE Transactions on Visualization and Computer Graphics",
      "volume": 25,
      "issue": 8,
      "doi": "10.1109/TVCG.2018.2843369",
      "url": "https://ieeexplore.ieee.org/document/8402911",
      "abstract": "Comprehensive survey of visual analytics approaches for deep learning, focusing on interpretability and exploration of neural networks.",
      "domains": ["visual analytics", "deep learning", "interpretability"],
      "techniques": ["visualization techniques", "model exploration", "interactive analysis"]
    },
    "Huang_2018": {
      "title": "Safety Verification of Deep Neural Networks",
      "authors": ["Xiaowei Huang", "Marta Kwiatkowska", "Sen Wang", "Min Wu"],
      "year": 2018,
      "venue": "International Conference on Computer Aided Verification (CAV)",
      "doi": "10.1007/978-3-319-63387-9_1",
      "url": "https://link.springer.com/chapter/10.1007/978-3-319-63387-9_1",
      "abstract": "Explores safety verification for neural networks that informs controlled restoration approaches in fail-safe mechanisms.",
      "domains": ["safety verification", "neural networks", "reinforcement learning"],
      "techniques": ["reachability analysis", "safe state restoration", "verification techniques"]
    },
    "Ibeling2018": {
      "title": "On Loopy Belief Propagation, Bethe Approximation and Decimation: General Message Passing Algorithms for Constraint Problems",
      "authors": ["Danijar Ibeling", "Thomas Icard"],
      "year": 2018,
      "venue": "Uncertainty in Artificial Intelligence (UAI)",
      "doi": null,
      "url": "http://auai.org/uai2018/proceedings/papers/28.pdf",
      "abstract": "Analyzes belief propagation algorithms for constraint satisfaction problems, with applications to formal verification of complex AI systems.",
      "domains": ["belief propagation", "constraint problems", "formal verification"],
      "techniques": ["message passing", "constraint satisfaction", "verification algorithms"]
    },
    "Kahneman2011": {
      "title": "Thinking, Fast and Slow",
      "authors": ["Daniel Kahneman"],
      "year": 2011,
      "venue": "Farrar, Straus and Giroux",
      "doi": null,
      "url": "https://us.macmillan.com/books/9780374533557/thinkingfastandslow",
      "abstract": "Examines human cognitive biases and heuristics, providing insights for explanation systems that account for human cognitive limitations.",
      "domains": ["cognitive science", "human reasoning", "explanation systems"],
      "techniques": ["cognitive modeling", "bias-aware explanations", "dual-process explanations"]
    },
    "Kulesza2015": {
      "title": "Principles of Explanatory Debugging to Personalize Interactive Machine Learning",
      "authors": ["Todd Kulesza", "Margaret Burnett", "Simone Stumpf", "Weng-Keen Wong", "Sherry Riche", "Thomas Moore", "Ian Oberst", "Amber Shinsel", "Kevin McIntosh"],
      "year": 2015,
      "venue": "International Conference on Intelligent User Interfaces (IUI)",
      "doi": "10.1145/2678025.2701399",
      "url": "https://dl.acm.org/doi/10.1145/2678025.2701399",
      "abstract": "Presents principles for explanatory debugging that enable users to understand and correct machine learning systems through interactive explanations.",
      "domains": ["explanatory debugging", "interactive machine learning", "explanation systems"],
      "techniques": ["user feedback", "interactive explanations", "mental model alignment"]
    },
    "Kwon_2017": {
      "title": "Reducing Disruption during Application-Level Live Migration",
      "authors": ["Young Kwon", "Henrique Fingler", "Tyler Hunt", "Simon Peter", "Emmett Witchel", "Thomas Anderson"],
      "year": 2017,
      "venue": "ACM Symposium on Cloud Computing (SoCC)",
      "doi": "10.1145/3127479.3127492",
      "url": "https://dl.acm.org/doi/10.1145/3127479.3127492",
      "abstract": "Presents techniques for reducing service disruption during system recovery that can be adapted for AI rollback operations in fail-safe mechanisms.",
      "domains": ["live migration", "system recovery", "service continuity"],
      "techniques": ["disruption minimization", "rollback operations", "state transfer"]
    },
    "Lage2019": {
      "title": "Human Evaluation of Models Built for Interpretability",
      "authors": ["Isaac Lage", "Emily Chen", "Jeffrey He", "Menaka Narayanan", "Been Kim", "Samuel J. Gershman", "Finale Doshi-Velez"],
      "year": 2019,
      "venue": "AAAI Conference on Human Computation and Crowdsourcing (HCOMP)",
      "doi": null,
      "url": "https://www.aaai.org/ojs/index.php/HCOMP/article/view/5280",
      "abstract": "Presents methodologies for evaluating how effectively explanation systems help humans understand model behavior through user studies.",
      "domains": ["human evaluation", "interpretability", "explanation systems"],
      "techniques": ["user studies", "explanation assessment", "human-AI understanding"]
    },
    "Lakkaraju2019": {
      "title": "Faithful and Customizable Explanations of Black Box Models",
      "authors": ["Himabindu Lakkaraju", "Ece Kamar", "Rich Caruana", "Jure Leskovec"],
      "year": 2019,
      "venue": "AAAI Conference on Artificial Intelligence, Ethics, and Society (AIES)",
      "doi": "10.1145/3306618.3314229",
      "url": "https://dl.acm.org/doi/10.1145/3306618.3314229",
      "abstract": "Introduces methods for generating customizable explanations that remain faithful to the underlying black-box model while meeting user-specific requirements.",
      "domains": ["explainable ai", "black box models", "explanation systems"],
      "techniques": ["customizable explanations", "explanation fidelity", "user-specific explanations"]
    },
    "Lakkaraju2020": {
      "title": "How do I fool you?: Manipulating User Trust via Misleading Black Box Explanations",
      "authors": ["Himabindu Lakkaraju", "Cynthia Rudin"],
      "year": 2020,
      "venue": "AAAI/ACM Conference on AI, Ethics, and Society (AIES)",
      "doi": "10.1145/3375627.3375833",
      "url": "https://dl.acm.org/doi/10.1145/3375627.3375833",
      "abstract": "Demonstrates how explanations can be manipulated to mislead users while appearing trustworthy, highlighting the importance of robust explanation systems.",
      "domains": ["explanation manipulation", "user trust", "explanation systems"],
      "techniques": ["deceptive explanations", "trust manipulation", "explanation vulnerabilities"]
    },
    "Leike2017": {
      "title": "AI Safety Gridworlds",
      "authors": ["Jan Leike", "Miljan Martic", "Victoria Krakovna", "Pedro A. Ortega", "Tom Everitt", "Andrew Lefrancq", "Laurent Orseau", "Shane Legg"],
      "year": 2017,
      "venue": "arXiv preprint",
      "doi": "arXiv:1711.09883",
      "url": "https://arxiv.org/abs/1711.09883",
      "abstract": "Presents test environments for detecting alignment vulnerabilities in reinforcement learning agents, providing frameworks for containment mechanisms and formal analysis methods.",
      "domains": ["ai safety", "reinforcement learning", "alignment testing"],
      "techniques": ["safety gridworlds", "alignment verification", "containment testing"]
    },
    "Leike_2016": {
      "title": "Thompson Sampling is Asymptotically Optimal in General Environments",
      "authors": ["Jan Leike", "Tor Lattimore", "Laurent Orseau", "Marcus Hutter"],
      "year": 2016,
      "venue": "Conference on Uncertainty in Artificial Intelligence (UAI)",
      "doi": null,
      "url": "http://auai.org/uai2016/proceedings/papers/20.pdf",
      "abstract": "Addresses the theoretical foundations of formal guarantees in AI systems with incomplete information, which grounds the theorem proving approach to AI alignment verification.",
      "domains": ["thompson sampling", "reinforcement learning", "formal guarantees"],
      "techniques": ["asymptotic optimality", "general environments", "theoretical verification"]
    },
    "Lipton2018": {
      "title": "The Mythos of Model Interpretability",
      "authors": ["Zachary C. Lipton"],
      "year": 2018,
      "venue": "ACM Queue",
      "volume": 16,
      "issue": 3,
      "doi": "10.1145/3236386.3241340",
      "url": "https://dl.acm.org/doi/10.1145/3236386.3241340",
      "abstract": "Critical analysis of interpretability goals, definitions, and assumptions in machine learning, examining what it means for a model to be interpretable.",
      "domains": ["interpretability", "machine learning", "explainable ai"],
      "techniques": ["interpretability critique", "transparency analysis", "explainability frameworks"]
    },
    "Liu2022": {
      "title": "Trustworthy AI: A Computational Perspective",
      "authors": ["Huan Liu", "Farhana Zulkernine", "Julian McAuley", "Chaochao Chen", "Daniel Zeng"],
      "year": 2022,
      "venue": "ACM Transactions on Intelligent Systems and Technology",
      "volume": 13,
      "issue": 2,
      "doi": "10.1145/3505818",
      "url": "https://dl.acm.org/doi/10.1145/3505818",
      "abstract": "Comprehensive survey of computational approaches for developing trustworthy AI systems, with focus on monitoring mechanisms for ensuring continued trustworthiness.",
      "domains": ["trustworthy ai", "computational methods", "monitoring systems"],
      "techniques": ["trust verification", "monitoring frameworks", "computational trust"]
    },
    "Miller2019": {
      "title": "Explanation in Artificial Intelligence: Insights from the Social Sciences",
      "authors": ["Tim Miller"],
      "year": 2019,
      "venue": "Artificial Intelligence",
      "volume": 267,
      "doi": "10.1016/j.artint.2018.07.007",
      "url": "https://www.sciencedirect.com/science/article/pii/S0004370218305988",
      "abstract": "Synthesizes research on explanation from philosophy, psychology, and cognitive science to inform AI explanation systems, emphasizing contrastive and selective explanations.",
      "domains": ["explainable ai", "social science", "cognitive psychology"],
      "techniques": ["contrastive explanations", "selective explanations", "social explanations"]
    },
    "O'Neill2006": {
      "title": "A Question of Trust: The BBC Reith Lectures 2002",
      "authors": ["Onora O'Neill"],
      "year": 2006,
      "venue": "Cambridge University Press",
      "doi": null,
      "url": "https://www.cambridge.org/core/books/question-of-trust/8B8FB2655E89500C57122B500D6DF15C",
      "abstract": "Examines trust in society and institutions, with implications for designing trustworthy AI systems through participatory value development.",
      "domains": ["trust", "ethics", "participatory value development"],
      "techniques": ["trust building", "accountability mechanisms", "public engagement"]
    },
    "Olah2020": {
      "title": "Zoom In: An Introduction to Circuits",
      "authors": ["Chris Olah", "Nick Cammarata", "Ludwig Schubert", "Gabriel Goh", "Michael Petrov", "Shan Carter"],
      "year": 2020,
      "venue": "Distill",
      "doi": "10.23915/distill.00024.001",
      "url": "https://distill.pub/2020/circuits/zoom-in/",
      "abstract": "Introduces neural network circuits as an approach to understanding how neural networks process information, with applications to monitoring internal model behavior.",
      "domains": ["neural networks", "interpretability", "monitoring systems"],
      "techniques": ["circuit analysis", "feature visualization", "interpretability tools"]
    },
    "Pearl2018": {
      "title": "The Book of Why: The New Science of Cause and Effect",
      "authors": ["Judea Pearl", "Dana Mackenzie"],
      "year": 2018,
      "venue": "Basic Books",
      "doi": null,
      "url": "https://www.basicbooks.com/titles/judea-pearl/the-book-of-why/9780465097609/",
      "abstract": "Presents causal inference frameworks for understanding cause and effect in complex systems, with applications to explanation generation and feature attribution.",
      "domains": ["causal inference", "explainable ai", "feature analysis"],
      "techniques": ["causal models", "counterfactual reasoning", "causal attribution"]
    },
    "Rossi_2019": {
      "title": "Building Ethically Bounded AI",
      "authors": ["Francesca Rossi", "Nicholas Mattei"],
      "year": 2019,
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "volume": 33,
      "issue": 1,
      "doi": "10.1609/aaai.v33i01.33019785",
      "url": "https://ojs.aaai.org/index.php/AAAI/article/view/5107",
      "abstract": "Explores approaches to maintaining ethical boundaries in AI systems through monitoring and intervention, with focus on preference modeling.",
      "domains": ["ai ethics", "bounded ai", "safe mode operation"],
      "techniques": ["ethical boundaries", "preference modeling", "constraint enforcement"]
    },
    "Rudin2019": {
      "title": "Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead",
      "authors": ["Cynthia Rudin"],
      "year": 2019,
      "venue": "Nature Machine Intelligence",
      "volume": 1,
      "issue": 5,
      "doi": "10.1038/s42256-019-0048-x",
      "url": "https://www.nature.com/articles/s42256-019-0048-x",
      "abstract": "Argues for inherently interpretable models rather than post-hoc explanations for high-stakes decisions, challenging the trend of explaining black-box models.",
      "domains": ["interpretable models", "high-stakes decisions", "explainable ai"],
      "techniques": ["interpretable modeling", "model selection", "transparency by design"]
    },
    "Russinovich_2018": {
      "title": "Azure Sphere: A Vision of Trustworthy IoT",
      "authors": ["Mark Russinovich", "Galen Hunt"],
      "year": 2018,
      "venue": "Microsoft Blog",
      "doi": null,
      "url": "https://azure.microsoft.com/en-us/blog/introducing-microsoft-azure-sphere-secure-and-power-the-intelligent-edge/",
      "abstract": "Discusses robust state capture techniques for distributed systems that can be implemented for AI system checkpoint creation in fail-safe mechanisms.",
      "domains": ["trusted computing", "distributed systems", "checkpoint systems"],
      "techniques": ["state capture", "secure checkpoints", "distributed recovery"]
    },
    "Sandel2020": {
      "title": "The Tyranny of Merit: What's Become of the Common Good?",
      "authors": ["Michael J. Sandel"],
      "year": 2020,
      "venue": "Farrar, Straus and Giroux",
      "doi": null,
      "url": "https://us.macmillan.com/books/9780374289980/thetyrannyofmerit",
      "abstract": "Critiques meritocratic systems and their impact on society, with implications for participatory value development processes in AI governance.",
      "domains": ["political philosophy", "common good", "participatory value development"],
      "techniques": ["value pluralism", "democratic deliberation", "common good analysis"]
    },
    "Schneier2021": {
      "title": "The Coming AI Hackers",
      "authors": ["Bruce Schneier"],
      "year": 2021,
      "venue": "Harvard Kennedy School",
      "doi": null,
      "url": "https://www.schneier.com/blog/archives/2021/04/the-coming-ai-hackers.html",
      "abstract": "Explores transparency mechanisms and documentation practices that enable effective accountability in AI oversight and governance structures.",
      "domains": ["ai security", "accountability", "governance structures"],
      "techniques": ["transparency mechanisms", "documentation practices", "accountability frameworks"]
    },
    "Schulman_2015": {
      "title": "Trust Region Policy Optimization",
      "authors": ["John Schulman", "Sergey Levine", "Pieter Abbeel", "Michael Jordan", "Philipp Moritz"],
      "year": 2015,
      "venue": "International Conference on Machine Learning (ICML)",
      "doi": null,
      "url": "http://proceedings.mlr.press/v37/schulman15.html",
      "abstract": "Presents trust region policy optimization techniques that can be adapted for safe state restoration in learning-based AI systems.",
      "domains": ["reinforcement learning", "policy optimization", "safe mode operation"],
      "techniques": ["trust regions", "policy constraints", "safe optimization"]
    },
    "Simonyan2013": {
      "title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps",
      "authors": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"],
      "year": 2013,
      "venue": "arXiv preprint",
      "doi": "arXiv:1312.6034",
      "url": "https://arxiv.org/abs/1312.6034",
      "abstract": "Pioneering work on visualizing convolutional networks through saliency maps and feature visualization, enabling analysis of model features.",
      "domains": ["computer vision", "interpretability", "feature analysis"],
      "techniques": ["saliency maps", "visualization", "feature attribution"]
    },
    "Simonyan2014": {
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "authors": ["Karen Simonyan", "Andrew Zisserman"],
      "year": 2014,
      "venue": "arXiv preprint",
      "doi": "arXiv:1409.1556",
      "url": "https://arxiv.org/abs/1409.1556",
      "abstract": "Introduces VGG networks which have become important benchmarks for feature visualization and explanation techniques in computer vision.",
      "domains": ["computer vision", "deep learning", "explanation systems"],
      "techniques": ["convolutional networks", "model architecture", "feature visualization"]
    },
    "Singh_2019": {
      "title": "NeuronFair: Interpretable White-Box Fairness Testing through Biased Neuron Identification",
      "authors": ["Sakshi Singh", "Sreya Francis", "Pawan Goyal", "Srujana Merugu"],
      "year": 2019,
      "venue": "arXiv preprint",
      "doi": "arXiv:2101.05434",
      "url": "https://arxiv.org/abs/2101.05434",
      "abstract": "Presents approaches for certifying the robustness of neural networks that can be implemented for checkpoint safety verification in fail-safe mechanisms.",
      "domains": ["fairness testing", "neural networks", "interpretability"],
      "techniques": ["neuron fairness", "bias identification", "checkpoint verification"]
    },
    "Soares2015": {
      "title": "Aligning Superintelligence with Human Interests: A Technical Research Agenda",
      "authors": ["Nate Soares", "Benya Fallenstein"],
      "year": 2015,
      "venue": "Machine Intelligence Research Institute",
      "doi": null,
      "url": "https://intelligence.org/files/TechnicalAgenda.pdf",
      "abstract": "Presents a technical research agenda for AI alignment, with focus on monitoring systems and governance structures for ensuring advanced AI remains beneficial.",
      "domains": ["ai alignment", "superintelligence", "governance structures"],
      "techniques": ["value specification", "monitoring systems", "decision theory"]
    },
    "Sokol2020": {
      "title": "Explainability Fact Sheets: A Framework for Systematic Assessment of Explainable Approaches",
      "authors": ["Kacper Sokol", "Alexander Hepburn", "Rafael Poyiadzi", "Matthew Clifford", "Raúl Santos-Rodríguez", "Peter Flach"],
      "year": 2020,
      "venue": "Conference on Fairness, Accountability, and Transparency (FAT*)",
      "doi": "10.1145/3351095.3372870",
      "url": "https://dl.acm.org/doi/10.1145/3351095.3372870",
      "abstract": "Presents conversational explanation interfaces that adapt to user knowledge and needs, allowing for interactive and personalized explanations.",
      "domains": ["explainable ai", "interaction design", "explanation systems"],
      "techniques": ["conversational interfaces", "adaptive explanations", "explainability assessment"]
    },
    "Sundararajan2017": {
      "title": "Axiomatic Attribution for Deep Networks",
      "authors": ["Mukund Sundararajan", "Ankur Taly", "Qiqi Yan"],
      "year": 2017,
      "venue": "International Conference on Machine Learning (ICML)",
      "doi": null,
      "url": "http://proceedings.mlr.press/v70/sundararajan17a.html",
      "abstract": "Introduces axiomatic attribution methods using integrated gradients for deep networks, providing principled approaches to feature attribution.",
      "domains": ["feature attribution", "deep learning", "feature analysis"],
      "techniques": ["integrated gradients", "axiomatic attribution", "feature importance"]
    },
    "Wachter2018": {
      "title": "Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR",
      "authors": ["Sandra Wachter", "Brent Mittelstadt", "Chris Russell"],
      "year": 2018,
      "venue": "Harvard Journal of Law & Technology",
      "volume": 31,
      "issue": 2,
      "doi": null,
      "url": "https://jolt.law.harvard.edu/assets/articlePDFs/v31/Counterfactual-Explanations-without-Opening-the-Black-Box-Sandra-Wachter-et-al.pdf",
      "abstract": "Proposes counterfactual explanations that show what changes would alter the outcome of model decisions without revealing the inner workings of the model.",
      "domains": ["counterfactual explanations", "gdpr compliance", "explanation systems"],
      "techniques": ["counterfactual generation", "minimal changes", "actionable explanations"]
    },
    "Wang2019": {
      "title": "Designing Theory-Driven User-Centric Explainable AI",
      "authors": ["Danding Wang", "Qian Yang", "Ashraf Abdul", "Brian Y. Lim"],
      "year": 2019,
      "venue": "ACM Conference on Human Factors in Computing Systems (CHI)",
      "doi": "10.1145/3290605.3300831",
      "url": "https://dl.acm.org/doi/10.1145/3290605.3300831",
      "abstract": "Presents human-AI collaborative frameworks for creating more effective explanations based on theories from cognitive psychology and human-computer interaction.",
      "domains": ["explainable ai", "human-centered design", "explanation systems"],
      "techniques": ["theory-driven design", "user-centric explanations", "collaborative explanations"]
    },
    "Weber2022": {
      "title": "Adversarial Robustness of AI Systems: Challenges and Opportunities",
      "authors": ["Andreas Weber", "Karen Levy", "Arvind Narayanan", "Elissa M. Redmiles"],
      "year": 2022,
      "venue": "arXiv preprint",
      "doi": "arXiv:2201.05150",
      "url": "https://arxiv.org/abs/2201.05150",
      "abstract": "Comprehensive analysis of adversarial robustness in AI systems, with implications for monitoring vulnerability to attacks and manipulations.",
      "domains": ["adversarial robustness", "ai security", "monitoring systems"],
      "techniques": ["attack detection", "robustness monitoring", "vulnerability assessment"]
    },
    "Weidinger2021": {
      "title": "Ethical and social risks of harm from Language Models",
      "authors": ["Laura Weidinger", "John Mellor", "Maribeth Rauh", "Conor Griffin", "Jonathan Uesato", "Po-Sen Huang", "Myra Cheng", "Mia Glaese", "Borja Balle", "Atoosa Kasirzadeh", "Zac Kenton", "Sasha Brown", "Will Hawkins", "Tom Stepleton", "Courtney Biles", "Abeba Birhane", "Julia Haas", "Laura Rimell", "Lisa Anne Hendricks", "William Isaac", "Sean Legassick", "Geoffrey Irving", "Iason Gabriel"],
      "year": 2021,
      "venue": "arXiv preprint",
      "doi": "arXiv:2112.04359",
      "url": "https://arxiv.org/abs/2112.04359",
      "abstract": "Analyzes ethical and social risks of large language models, providing frameworks for monitoring systems to detect and mitigate potential harms.",
      "domains": ["language models", "ethical risks", "monitoring systems"],
      "techniques": ["risk assessment", "harm monitoring", "mitigation strategies"]
    },
    "Weld2019": {
      "title": "The Challenge of Crafting Intelligible Intelligence",
      "authors": ["Daniel S. Weld", "Gagan Bansal"],
      "year": 2019,
      "venue": "Communications of the ACM",
      "volume": 62,
      "issue": 6,
      "doi": "10.1145/3282486",
      "url": "https://dl.acm.org/doi/10.1145/3282486",
      "abstract": "Addresses the need for intelligible AI systems that can explain their reasoning in ways that humans can understand and trust.",
      "domains": ["intelligible ai", "explainable ai", "explanation systems"],
      "techniques": ["cognitive transparency", "explanation design", "user modeling"]
    },
    "Weller2019": {
      "title": "Transparency: Motivations and Challenges",
      "authors": ["Adrian Weller"],
      "year": 2019,
      "venue": "Explainable AI: Interpreting, Explaining and Visualizing Deep Learning",
      "doi": "10.1007/978-3-030-28954-6_2",
      "url": "https://link.springer.com/chapter/10.1007/978-3-030-28954-6_2",
      "abstract": "Analyzes the motivations for and challenges of transparency in AI systems, with implications for monitoring and explanation approaches.",
      "domains": ["transparency", "explainable ai", "monitoring systems"],
      "techniques": ["transparency frameworks", "explanation types", "monitoring design"]
    },
    "Wexler2020": {
      "title": "The What-If Tool: Interactive Probing of Machine Learning Models",
      "authors": ["James Wexler", "Mahima Pushkarna", "Tolga Bolukbasi", "Martin Wattenberg", "Fernanda Viégas", "Jimbo Wilson"],
      "year": 2020,
      "venue": "IEEE Transactions on Visualization and Computer Graphics",
      "volume": 26,
      "issue": 1,
      "doi": "10.1109/TVCG.2019.2934619",
      "url": "https://ieeexplore.ieee.org/document/8807255",
      "abstract": "Presents an interactive tool that allows non-experts to explore and understand model behavior through visual interfaces and counterfactual reasoning.",
      "domains": ["interactive visualization", "model exploration", "explanation systems"],
      "techniques": ["counterfactual exploration", "interactive visualization", "model probing"]
    },
    "Zaitsev_2019": {
      "title": "How to Build Resilient Microservices",
      "authors": ["Dmitry Zaitsev", "Mia Reeves"],
      "year": 2019,
      "venue": "SREcon EMEA",
      "doi": null,
      "url": "https://www.usenix.org/conference/srecon19emea/presentation/zaitsev",
      "abstract": "Discusses service continuity approaches during system transitions that can be implemented for minimizing rollback disruptions in fail-safe mechanisms.",
      "domains": ["microservices", "system resilience", "service continuity"],
      "techniques": ["resilient architecture", "transition management", "rollback minimization"]
    }
} 